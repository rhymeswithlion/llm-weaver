{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-end notebook!\n",
    "\n",
    "Here is the workflow:\n",
    "* Sampling configs (sampling parameters, etc.) lead to...\n",
    "* Weaving configs (blank model settings, donor model settings, layer assignments) lead to...\n",
    "* Models (probably TFRobertaForSequenceClassification in all cases) lead to...\n",
    "* Performance scores (numbers from 0-100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Sampling configs to weaving configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Weaving configs to models\n",
    "\n",
    "The weaving config needs to define the following:\n",
    "* `blank_model_config`\n",
    "* `layer_assignments`\n",
    "* `embedding_assignments`\n",
    "* `classification_head_assignments`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Some of the functions we need\n",
    "from transformers.models.roberta.modeling_tf_roberta import TFRobertaForSequenceClassification\n",
    "from transformers import RobertaConfig\n",
    "\n",
    "def get_model(model_str):\n",
    "    from transformers import TFRobertaForSequenceClassification\n",
    "    model = TFRobertaForSequenceClassification.from_pretrained(model_str, from_pt=True)\n",
    "    return model\n",
    "\n",
    "def get_model_config(model_str):\n",
    "    model = get_model(model_str)\n",
    "    config = model.config.to_dict()\n",
    "    del config[\"_name_or_path\"]\n",
    "    return config\n",
    "\n",
    "def get_blank_model(config):\n",
    "    blank_model = TFRobertaForSequenceClassification(\n",
    "        RobertaConfig(**config)\n",
    "    )\n",
    "    blank_model.build()\n",
    "    return blank_model\n",
    "\n",
    "def dict_overwrite(d1, d2):\n",
    "    d1 = d1.copy()\n",
    "    for k in d2:\n",
    "        d1[k] = d2[k]\n",
    "    return d1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'blank_model_config': {'return_dict': True,\n",
       "   'output_hidden_states': False,\n",
       "   'output_attentions': False,\n",
       "   'torchscript': False,\n",
       "   'torch_dtype': None,\n",
       "   'use_bfloat16': False,\n",
       "   'tf_legacy_loss': False,\n",
       "   'pruned_heads': {},\n",
       "   'tie_word_embeddings': True,\n",
       "   'is_encoder_decoder': False,\n",
       "   'is_decoder': False,\n",
       "   'cross_attention_hidden_size': None,\n",
       "   'add_cross_attention': False,\n",
       "   'tie_encoder_decoder': False,\n",
       "   'max_length': 20,\n",
       "   'min_length': 0,\n",
       "   'do_sample': False,\n",
       "   'early_stopping': False,\n",
       "   'num_beams': 1,\n",
       "   'num_beam_groups': 1,\n",
       "   'diversity_penalty': 0.0,\n",
       "   'temperature': 1.0,\n",
       "   'top_k': 50,\n",
       "   'top_p': 1.0,\n",
       "   'typical_p': 1.0,\n",
       "   'repetition_penalty': 1.0,\n",
       "   'length_penalty': 1.0,\n",
       "   'no_repeat_ngram_size': 0,\n",
       "   'encoder_no_repeat_ngram_size': 0,\n",
       "   'bad_words_ids': None,\n",
       "   'num_return_sequences': 1,\n",
       "   'chunk_size_feed_forward': 0,\n",
       "   'output_scores': False,\n",
       "   'return_dict_in_generate': False,\n",
       "   'forced_bos_token_id': None,\n",
       "   'forced_eos_token_id': None,\n",
       "   'remove_invalid_values': False,\n",
       "   'exponential_decay_length_penalty': None,\n",
       "   'suppress_tokens': None,\n",
       "   'begin_suppress_tokens': None,\n",
       "   'architectures': ['RobertaForSequenceClassification'],\n",
       "   'finetuning_task': 'glue:rte',\n",
       "   'id2label': {0: 'LABEL_0', 1: 'LABEL_1'},\n",
       "   'label2id': {'LABEL_0': 0, 'LABEL_1': 1},\n",
       "   'tokenizer_class': None,\n",
       "   'prefix': None,\n",
       "   'bos_token_id': 0,\n",
       "   'pad_token_id': 1,\n",
       "   'eos_token_id': 2,\n",
       "   'sep_token_id': None,\n",
       "   'decoder_start_token_id': None,\n",
       "   'task_specific_params': None,\n",
       "   'problem_type': None,\n",
       "   'transformers_version': '4.35.0',\n",
       "   'gradient_checkpointing': False,\n",
       "   'model_type': 'roberta',\n",
       "   'vocab_size': 50265,\n",
       "   'hidden_size': 768,\n",
       "   'num_hidden_layers': 12,\n",
       "   'num_attention_heads': 12,\n",
       "   'hidden_act': 'gelu',\n",
       "   'intermediate_size': 3072,\n",
       "   'hidden_dropout_prob': 0.1,\n",
       "   'attention_probs_dropout_prob': 0.1,\n",
       "   'max_position_embeddings': 514,\n",
       "   'type_vocab_size': 1,\n",
       "   'initializer_range': 0.02,\n",
       "   'layer_norm_eps': 1e-05,\n",
       "   'position_embedding_type': 'absolute',\n",
       "   'use_cache': True,\n",
       "   'classifier_dropout': None},\n",
       "  'layer_assignments': [{'type': 'SingleLayer',\n",
       "    'params': {'donor': 'textattack/roberta-base-RTE',\n",
       "     'hidden_layer_number': 0}},\n",
       "   {'type': 'SingleLayer',\n",
       "    'params': {'donor': 'textattack/roberta-base-RTE',\n",
       "     'hidden_layer_number': 1}},\n",
       "   {'type': 'SingleLayer',\n",
       "    'params': {'donor': 'textattack/roberta-base-RTE',\n",
       "     'hidden_layer_number': 2}},\n",
       "   {'type': 'SingleLayer',\n",
       "    'params': {'donor': 'textattack/roberta-base-RTE',\n",
       "     'hidden_layer_number': 3}},\n",
       "   {'type': 'SingleLayer',\n",
       "    'params': {'donor': 'textattack/roberta-base-RTE',\n",
       "     'hidden_layer_number': 4}},\n",
       "   {'type': 'SingleLayer',\n",
       "    'params': {'donor': 'textattack/roberta-base-RTE',\n",
       "     'hidden_layer_number': 5}},\n",
       "   {'type': 'SingleLayer',\n",
       "    'params': {'donor': 'textattack/roberta-base-RTE',\n",
       "     'hidden_layer_number': 6}},\n",
       "   {'type': 'SingleLayer',\n",
       "    'params': {'donor': 'textattack/roberta-base-RTE',\n",
       "     'hidden_layer_number': 7}},\n",
       "   {'type': 'SingleLayer',\n",
       "    'params': {'donor': 'textattack/roberta-base-RTE',\n",
       "     'hidden_layer_number': 8}},\n",
       "   {'type': 'SingleLayer',\n",
       "    'params': {'donor': 'textattack/roberta-base-RTE',\n",
       "     'hidden_layer_number': 9}},\n",
       "   {'type': 'SingleLayer',\n",
       "    'params': {'donor': 'textattack/roberta-base-RTE',\n",
       "     'hidden_layer_number': 10}},\n",
       "   {'type': 'SingleLayer',\n",
       "    'params': {'donor': 'textattack/roberta-base-RTE',\n",
       "     'hidden_layer_number': 11}}],\n",
       "  'classification_head': {'type': 'SingleClassificationHead',\n",
       "   'params': {'donor': 'textattack/roberta-base-RTE'}},\n",
       "  'embeddings': {'type': 'SingleEmbeddings',\n",
       "   'params': {'donor': 'textattack/roberta-base-RTE'}}}]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This defines a placeholder weaving_configs variable. It will be replaced by the actual weaving\n",
    "# configs from the previous step.\n",
    "\n",
    "import random\n",
    "from random import randint\n",
    "# Generate model weaving configuration\n",
    "\n",
    "\n",
    "weaving_configs = [{\n",
    "    # The task (i.e. the classification head output size should match the task at hand)\n",
    "    \"blank_model_config\": dict_overwrite(\n",
    "        get_model_config(\"textattack/roberta-base-RTE\"),\n",
    "        {\n",
    "            \"num_hidden_layers\": 12,\n",
    "        }\n",
    "    ),\n",
    "    # Layer assignments   \n",
    "    \"layer_assignments\": [\n",
    "        {\n",
    "            \"type\": \"SingleLayer\",\n",
    "            \"params\": {\n",
    "                # Load donor model\n",
    "                \"donor\": \"textattack/roberta-base-RTE\",\n",
    "                # Pick a layer\n",
    "                \"hidden_layer_number\": i,\n",
    "            },\n",
    "        } for i in range(12)\n",
    "    ],\n",
    "    # The head (i.e. the classification head should match the task at hand)\n",
    "    # THESE ARE DIFFERENT BETWEEN RTE AND MNLI\n",
    "    \"classification_head\": {\n",
    "        \"type\": \"SingleClassificationHead\",\n",
    "        \"params\": {\n",
    "            \"donor\": \"textattack/roberta-base-RTE\",\n",
    "        }\n",
    "    },\n",
    "    # The embeddings layer\n",
    "    # THESE ARE DIFFERENT BETWEEN RTE AND MNLI\n",
    "    \"embeddings\": {\n",
    "        \"type\": \"SingleEmbeddings\",\n",
    "        \"params\": {\n",
    "            \"donor\": \"textattack/roberta-base-RTE\",\n",
    "        }\n",
    "    },\n",
    "\n",
    "}]\n",
    "\n",
    "# There's just one config in here\n",
    "weaving_configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading textattack/roberta-base-RTE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "TODO: Kirthi",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/briancruz/2023-fall-cs-194-294-merging-llms/notebooks/end-to-end-BC-2023-11-23.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/briancruz/2023-fall-cs-194-294-merging-llms/notebooks/end-to-end-BC-2023-11-23.ipynb#X34sZmlsZQ%3D%3D?line=110'>111</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m target_model\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/briancruz/2023-fall-cs-194-294-merging-llms/notebooks/end-to-end-BC-2023-11-23.ipynb#X34sZmlsZQ%3D%3D?line=115'>116</a>\u001b[0m weaved_models \u001b[39m=\u001b[39m (\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/briancruz/2023-fall-cs-194-294-merging-llms/notebooks/end-to-end-BC-2023-11-23.ipynb#X34sZmlsZQ%3D%3D?line=116'>117</a>\u001b[0m     weave_models(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mweaving_config)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/briancruz/2023-fall-cs-194-294-merging-llms/notebooks/end-to-end-BC-2023-11-23.ipynb#X34sZmlsZQ%3D%3D?line=117'>118</a>\u001b[0m     \u001b[39mfor\u001b[39;00m weaving_config \u001b[39min\u001b[39;00m weaving_configs\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/briancruz/2023-fall-cs-194-294-merging-llms/notebooks/end-to-end-BC-2023-11-23.ipynb#X34sZmlsZQ%3D%3D?line=118'>119</a>\u001b[0m )\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/briancruz/2023-fall-cs-194-294-merging-llms/notebooks/end-to-end-BC-2023-11-23.ipynb#X34sZmlsZQ%3D%3D?line=119'>120</a>\u001b[0m model \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(weaved_models)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/briancruz/2023-fall-cs-194-294-merging-llms/notebooks/end-to-end-BC-2023-11-23.ipynb#X34sZmlsZQ%3D%3D?line=120'>121</a>\u001b[0m model\u001b[39m.\u001b[39msummary()\n",
      "\u001b[1;32m/Users/briancruz/2023-fall-cs-194-294-merging-llms/notebooks/end-to-end-BC-2023-11-23.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/briancruz/2023-fall-cs-194-294-merging-llms/notebooks/end-to-end-BC-2023-11-23.ipynb#X34sZmlsZQ%3D%3D?line=108'>109</a>\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnknown embeddings type: \u001b[39m\u001b[39m{\u001b[39;00membeddings[\u001b[39m'\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/briancruz/2023-fall-cs-194-294-merging-llms/notebooks/end-to-end-BC-2023-11-23.ipynb#X34sZmlsZQ%3D%3D?line=110'>111</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m target_model\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/briancruz/2023-fall-cs-194-294-merging-llms/notebooks/end-to-end-BC-2023-11-23.ipynb#X34sZmlsZQ%3D%3D?line=115'>116</a>\u001b[0m weaved_models \u001b[39m=\u001b[39m (\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/briancruz/2023-fall-cs-194-294-merging-llms/notebooks/end-to-end-BC-2023-11-23.ipynb#X34sZmlsZQ%3D%3D?line=116'>117</a>\u001b[0m     weave_models(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mweaving_config)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/briancruz/2023-fall-cs-194-294-merging-llms/notebooks/end-to-end-BC-2023-11-23.ipynb#X34sZmlsZQ%3D%3D?line=117'>118</a>\u001b[0m     \u001b[39mfor\u001b[39;00m weaving_config \u001b[39min\u001b[39;00m weaving_configs\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/briancruz/2023-fall-cs-194-294-merging-llms/notebooks/end-to-end-BC-2023-11-23.ipynb#X34sZmlsZQ%3D%3D?line=118'>119</a>\u001b[0m )\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/briancruz/2023-fall-cs-194-294-merging-llms/notebooks/end-to-end-BC-2023-11-23.ipynb#X34sZmlsZQ%3D%3D?line=119'>120</a>\u001b[0m model \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(weaved_models)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/briancruz/2023-fall-cs-194-294-merging-llms/notebooks/end-to-end-BC-2023-11-23.ipynb#X34sZmlsZQ%3D%3D?line=120'>121</a>\u001b[0m model\u001b[39m.\u001b[39msummary()\n",
      "\u001b[1;32m/Users/briancruz/2023-fall-cs-194-294-merging-llms/notebooks/end-to-end-BC-2023-11-23.ipynb Cell 7\u001b[0m line \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/briancruz/2023-fall-cs-194-294-merging-llms/notebooks/end-to-end-BC-2023-11-23.ipynb#X34sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m \u001b[39mif\u001b[39;00m classification_head \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/briancruz/2023-fall-cs-194-294-merging-llms/notebooks/end-to-end-BC-2023-11-23.ipynb#X34sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m     \u001b[39mif\u001b[39;00m classification_head[\u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mSingleClassificationHead\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/briancruz/2023-fall-cs-194-294-merging-llms/notebooks/end-to-end-BC-2023-11-23.ipynb#X34sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m         \u001b[39m# We want to copy weights from the donor model to the target model. There are four parts.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/briancruz/2023-fall-cs-194-294-merging-llms/notebooks/end-to-end-BC-2023-11-23.ipynb#X34sZmlsZQ%3D%3D?line=84'>85</a>\u001b[0m         \u001b[39m# tf_roberta_for_sequence_classification_18/classifier/dense/kernel:0 (768, 768)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/briancruz/2023-fall-cs-194-294-merging-llms/notebooks/end-to-end-BC-2023-11-23.ipynb#X34sZmlsZQ%3D%3D?line=89'>90</a>\u001b[0m         \u001b[39m# and need to go to target_model.classifier.weights\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/briancruz/2023-fall-cs-194-294-merging-llms/notebooks/end-to-end-BC-2023-11-23.ipynb#X34sZmlsZQ%3D%3D?line=90'>91</a>\u001b[0m         \u001b[39m# using something like target_weight.assign(source_weight.numpy())\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/briancruz/2023-fall-cs-194-294-merging-llms/notebooks/end-to-end-BC-2023-11-23.ipynb#X34sZmlsZQ%3D%3D?line=91'>92</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTODO: Kirthi\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/briancruz/2023-fall-cs-194-294-merging-llms/notebooks/end-to-end-BC-2023-11-23.ipynb#X34sZmlsZQ%3D%3D?line=92'>93</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/briancruz/2023-fall-cs-194-294-merging-llms/notebooks/end-to-end-BC-2023-11-23.ipynb#X34sZmlsZQ%3D%3D?line=93'>94</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnknown classification head type: \u001b[39m\u001b[39m{\u001b[39;00mclassification_head[\u001b[39m'\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: TODO: Kirthi"
     ]
    }
   ],
   "source": [
    "# The functions we need for model weaving\n",
    "\n",
    "from transformers import TFAutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "def get_model_and_tokenizer(identifier):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(identifier)\n",
    "    model = TFAutoModelForSequenceClassification.from_pretrained(identifier, from_pt=True)\n",
    "    return model, tokenizer\n",
    "\n",
    "def _get_layer_to_weights_map(model):\n",
    "    from collections import defaultdict\n",
    "    import re\n",
    "\n",
    "    layer_to_weights_map = defaultdict(dict)\n",
    "    for weight in model.weights:\n",
    "        matches = re.findall(r'/layer_._(\\d+)/', weight.name)\n",
    "        if not matches:\n",
    "            continue\n",
    "\n",
    "        layer_number = int(matches[0])\n",
    "        layer_to_weights_map[layer_number][weight.name.partition(f\"/layer_._{layer_number}/\")[-1]] = weight\n",
    "\n",
    "    return {\n",
    "        layer_number: dict(weights)\n",
    "        for layer_number, weights in layer_to_weights_map.items()\n",
    "    }\n",
    "    \n",
    "def assign_weights_from_one_layer_to_another(source_model, target_model, source_layer_number, target_layer_number):\n",
    "\n",
    "    # This part is recalculated often, but it's fast. In the future we could \n",
    "    # cache it in a class as a cached property, but we'll leave it here for now.\n",
    "    target_model_layer_to_weights_map = _get_layer_to_weights_map(target_model)\n",
    "    source_model_layer_to_weights_map = _get_layer_to_weights_map(source_model)\n",
    "\n",
    "    # Get the layer objects\n",
    "    source_layer = source_model_layer_to_weights_map[source_layer_number]\n",
    "    target_layer = target_model_layer_to_weights_map[target_layer_number]\n",
    "\n",
    "    # Make sure that all the suffixes match\n",
    "    assert set(source_layer.keys()) == set(target_layer.keys())\n",
    "\n",
    "    # Make sure that all the shapes match\n",
    "    for weight_name, weight_object in source_layer.items():\n",
    "        assert weight_object.shape == target_layer[weight_name].shape\n",
    "    \n",
    "    # Assign weights from one layer to another\n",
    "    for weight_name, weight_object in source_layer.items():\n",
    "        target_layer[weight_name].assign(weight_object.numpy())\n",
    "\n",
    "\n",
    "def weave_models(blank_model_config, layer_assignments, classification_head=None, embeddings=None):\n",
    "    # Create a blank model\n",
    "    target_model = get_blank_model(blank_model_config)\n",
    "\n",
    "    # We gather all the names of the donor models we need to load\n",
    "    source_model_names = set(\n",
    "        layer_assignment[\"params\"][\"donor\"]\n",
    "        for layer_assignment in layer_assignments\n",
    "    )\n",
    "    if classification_head is not None:\n",
    "        source_model_names.add(classification_head[\"params\"][\"donor\"])\n",
    "    if embeddings is not None:\n",
    "        source_model_names.add(embeddings[\"params\"][\"donor\"])\n",
    "\n",
    "    # We load all the donor models into a dictionary for easy access\n",
    "    source_models = {}\n",
    "    for source_model_name in source_model_names:\n",
    "        print(f\"Loading {source_model_name}\")\n",
    "        source_models[source_model_name] = get_model(source_model_name)\n",
    "\n",
    "    for layer_assignment in layer_assignments:\n",
    "        if layer_assignment[\"type\"] == \"SingleLayer\":\n",
    "            assign_weights_from_one_layer_to_another(\n",
    "                source_model=source_models[layer_assignment[\"params\"][\"donor\"]],\n",
    "                target_model=target_model,\n",
    "                source_layer_number=layer_assignment[\"params\"][\"hidden_layer_number\"],\n",
    "                target_layer_number=layer_assignment[\"params\"][\"hidden_layer_number\"]\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Unknown layer assignment type: {layer_assignment['type']}\")\n",
    "\n",
    "    if classification_head is not None:\n",
    "        if classification_head[\"type\"] == \"SingleClassificationHead\":\n",
    "            # We want to copy weights from the donor model to the target model. There are four parts.\n",
    "            # tf_roberta_for_sequence_classification_18/classifier/dense/kernel:0 (768, 768)\n",
    "            # tf_roberta_for_sequence_classification_18/classifier/dense/bias:0 (768,)\n",
    "            # tf_roberta_for_sequence_classification_18/classifier/out_proj/kernel:0 (768, 3)\n",
    "            # tf_roberta_for_sequence_classification_18/classifier/out_proj/bias:0 (3,)\n",
    "            # They live in source_models[classification_head[\"params\"][\"donor\"]].classifier.weights\n",
    "            # and need to go to target_model.classifier.weights\n",
    "            # using something like target_weight.assign(source_weight.numpy())\n",
    "            raise NotImplementedError(\"TODO: Kirthi\")\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Unknown classification head type: {classification_head['type']}\")\n",
    "        \n",
    "    if embeddings is not None:\n",
    "        if embeddings[\"type\"] == \"SingleEmbeddings\":\n",
    "            # We want to copy weights from the donor model to the target model. There are five parts.\n",
    "            # tf_roberta_for_sequence_classification_18/roberta/embeddings/word_embeddings/weight:0 (50265, 768)\n",
    "            # tf_roberta_for_sequence_classification_18/roberta/embeddings/token_type_embeddings/embeddings:0 (1, 768)\n",
    "            # tf_roberta_for_sequence_classification_18/roberta/embeddings/position_embeddings/embeddings:0 (514, 768)\n",
    "            # tf_roberta_for_sequence_classification_18/roberta/embeddings/LayerNorm/gamma:0 (768,)\n",
    "            # tf_roberta_for_sequence_classification_18/roberta/embeddings/LayerNorm/beta:0 (768,)\n",
    "            # They live in source_models[embeddings[\"params\"][\"donor\"]].roberta.embeddings.weights\n",
    "            # and need to go to target_model.roberta.embeddings.weights\n",
    "            # using something like target_weight.assign(source_weight.numpy())\n",
    "            raise NotImplementedError(\"TODO: Kirthi\")\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Unknown embeddings type: {embeddings['type']}\")\n",
    "\n",
    "    return target_model\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "weaved_models = (\n",
    "    weave_models(**weaving_config)\n",
    "    for weaving_config in weaving_configs\n",
    ")\n",
    "model = next(weaved_models)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the names of the variables we want to copy. Hope they are the same\n",
      "tf_roberta_for_sequence_classification_108/roberta/embeddings/word_embeddings/weight:0\n",
      "tf_roberta_for_sequence_classification_110/roberta/embeddings/word_embeddings/weight:0\n",
      "Before\n",
      "-0.007996951\n",
      "After\n",
      "0.14725289\n"
     ]
    }
   ],
   "source": [
    "# Kirthi: you can work with the models like this ðŸ‘€\n",
    "\n",
    "model_mnli = get_model(\"textattack/roberta-base-MNLI\")\n",
    "blank_model = get_blank_model(get_model_config(\"textattack/roberta-base-MNLI\"))\n",
    "\n",
    "print(\"the names of the variables we want to copy. Hope they are the same\")\n",
    "print(model_mnli.roberta.embeddings.weights[0].name)  # tf_roberta_for_sequence_classification_108/roberta/embeddings/word_embeddings/weight:0\n",
    "print(blank_model.roberta.embeddings.weights[0].name)  # tf_roberta_for_sequence_classification_110/roberta/embeddings/word_embeddings/weight:0\n",
    "\n",
    "print(\"Before\")\n",
    "print(blank_model.roberta.embeddings.weights[0].numpy()[0][0]) # -0.007996951\n",
    "\n",
    "# Copy the weights from the MNLI model to the blank model, \n",
    "blank_model.roberta.embeddings.weights[0].assign(model_mnli.roberta.embeddings.weights[0].numpy())\n",
    "\n",
    "print(\"After\")\n",
    "print(blank_model.roberta.embeddings.weights[0].numpy()[0][0]) # 0.14725289"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Models to scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'model_merging'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/marencordts/Documents/GitHub/2023-fall-cs-194-294-merging-llms/notebooks/end-to-end-idea.ipynb Cell 10\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/marencordts/Documents/GitHub/2023-fall-cs-194-294-merging-llms/notebooks/end-to-end-idea.ipynb#X12sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/marencordts/Documents/GitHub/2023-fall-cs-194-294-merging-llms/notebooks/end-to-end-idea.ipynb#X12sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m#from model_merging import data\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/marencordts/Documents/GitHub/2023-fall-cs-194-294-merging-llms/notebooks/end-to-end-idea.ipynb#X12sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmodel_merging\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_merging\u001b[39;00m \u001b[39mimport\u001b[39;00m evaluation\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/marencordts/Documents/GitHub/2023-fall-cs-194-294-merging-llms/notebooks/end-to-end-idea.ipynb#X12sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m#from model_merging.model_merging.evaluation import load_metric_for_glue_task, evaluate_model\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/marencordts/Documents/GitHub/2023-fall-cs-194-294-merging-llms/notebooks/end-to-end-idea.ipynb#X12sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m#from model_merging.evaluation import load_metric_for_glue_task, evaluate_model\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/marencordts/Documents/GitHub/2023-fall-cs-194-294-merging-llms/notebooks/end-to-end-idea.ipynb#X12sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/marencordts/Documents/GitHub/2023-fall-cs-194-294-merging-llms/notebooks/end-to-end-idea.ipynb#X12sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m#from model_merging import hdf5_util\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/marencordts/Documents/GitHub/2023-fall-cs-194-294-merging-llms/notebooks/end-to-end-idea.ipynb#X12sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m#from model_merging import merging\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/marencordts/Documents/GitHub/2023-fall-cs-194-294-merging-llms/notebooks/end-to-end-idea.ipynb#X12sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_to_tfds_task_name\u001b[39m(task, split):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'model_merging'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from absl import app, flags\n",
    "from transformers.data.processors import glue as hf_glue\n",
    "# Example import statement assuming 'evaluation' is a module within 'model_merging'\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "#flags.DEFINE_string(\"glue_task_weave_model\", \"rte\", \"GLUE task for evaluation\")\n",
    "#flags.DEFINE_string(\"split_weave_model\", \"validation\", \"Data split for evaluation\")\n",
    "#flags.DEFINE_integer(\"n_examples_weave_model\", 1000, \"Number of examples to evaluate\")\n",
    "#flags.DEFINE_integer(\"batch_size_weave_model\", 32, \"Batch size for evaluation\")\n",
    "#flags.DEFINE_integer(\"sequence_length_weave_model\", 128, \"Maximum sequence length\")\n",
    "#flags.DEFINE_string(\"favor_target_model_weave_model\", \"accuracy\", \"Favor target model based on the metric (e.g., 'accuracy')\")\n",
    "#flags.DEFINE_boolean(\"normalize_fishers_weave_model\", True, \"Normalize Fisher scores\")\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "#from model_merging import data\n",
    "from model_merging.model_merging import evaluation\n",
    "#from model_merging.model_merging.evaluation import load_metric_for_glue_task, evaluate_model\n",
    "#from model_merging.evaluation import load_metric_for_glue_task, evaluate_model\n",
    "\n",
    "#from model_merging import hdf5_util\n",
    "#from model_merging import merging\n",
    "\n",
    "def _to_tfds_task_name(task, split):\n",
    "    if task == \"sts-b\":\n",
    "        task = \"stsb\"\n",
    "    elif task == \"sst-2\":\n",
    "        task = \"sst2\"\n",
    "    elif task == \"mnli\" and split != \"train\":\n",
    "        task = \"mnli_matched\"\n",
    "    elif task == \"mnli-mm\" and split != \"train\":\n",
    "        task = \"mnli_mismatched\"\n",
    "    return task\n",
    "\n",
    "_STSB_MIN = 0\n",
    "_STSB_MAX = 5\n",
    "_STSB_NUM_BINS = 5 * (_STSB_MAX - _STSB_MIN)\n",
    "\n",
    "def _convert_dataset_to_features(\n",
    "    dataset,\n",
    "    tokenizer,\n",
    "    max_length,\n",
    "    task,\n",
    "):\n",
    "    \"\"\"Note that this is only for single examples; won't work with batched inputs.\"\"\"\n",
    "    pad_token = tokenizer.pad_token_id\n",
    "    # NOTE: Not sure if this is correct, but it matches up for BERT. RoBERTa does\n",
    "    # not appear to use token types\n",
    "    pad_token_segment_id = tokenizer.pad_token_type_id\n",
    "    _glue_processors = hf_glue.glue_processors\n",
    "    _glue_output_modes = hf_glue.glue_output_modes\n",
    "    processor = _glue_processors[task]()\n",
    "    output_mode = _glue_output_modes[task]\n",
    "\n",
    "    if task == \"sts-b\":\n",
    "        # STS-B regression\n",
    "        stsb_bins = np.linspace(_STSB_MIN, _STSB_MAX, num=_STSB_NUM_BINS + 1)\n",
    "        stsb_bins = stsb_bins[1:-1]\n",
    "    else:\n",
    "        label_list = processor.get_labels()\n",
    "        label_map = {label: i for i, label in enumerate(label_list)}\n",
    "\n",
    "\n",
    "def load_glue_dataset(task: str, split: str, tokenizer, max_length: int):\n",
    "    tfds_task = _to_tfds_task_name(task, split)\n",
    "    ds = tf.load(f\"glue/{tfds_task}\", split=split)\n",
    "    ds = _convert_dataset_to_features(\n",
    "        ds,\n",
    "        tokenizer,\n",
    "        max_length,\n",
    "        task,\n",
    "    )\n",
    "    return ds\n",
    "\n",
    "def main(_):\n",
    "    # Load the weaved model\n",
    "    weaved_model = TFRobertaForSequenceClassification.from_pretrained('weaved_model')\n",
    "\n",
    "    # Load the dataset\n",
    "    ds = load_glue_dataset(\n",
    "        task=FLAGS.glue_task,\n",
    "        split=FLAGS.split,\n",
    "        tokenizer=weaved_model.tokenizer,\n",
    "        max_length=FLAGS.sequence_length,\n",
    "    )\n",
    "    ds = ds.take(FLAGS.n_examples).batch(FLAGS.batch_size)\n",
    "\n",
    "    # Load metrics\n",
    "    metric = evaluation.load_metric_for_glue_task(FLAGS.glue_task)\n",
    "\n",
    "    # Evaluate the weaved model\n",
    "    results = evaluation.evaluate_model(weaved_model, ds, metric)\n",
    "\n",
    "    # Print evaluation results\n",
    "    print(80 * \"*\")\n",
    "    print(\" Weaved Model Evaluation\")\n",
    "    print(80 * \"*\")\n",
    "    print(f\"{FLAGS.glue_task} {FLAGS.split} {FLAGS.n_examples} Examples\")\n",
    "    print(80 * \"-\")\n",
    "    print(f\"Metric: {FLAGS.favor_target_model}\")\n",
    "    print(f\"{metric.name}: {results[FLAGS.favor_target_model]}\")\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "#    app.run(main)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
