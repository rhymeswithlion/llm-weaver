{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the weaving code on the base models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: /home/brian/2023-fall-cs-194-294-merging-llms/.venv/bin/pip: bad interpreter: .venv/bin/python3.8: no such file or directory\n",
      "zsh:1: /home/brian/2023-fall-cs-194-294-merging-llms/.venv/bin/pip: bad interpreter: .venv/bin/python3.8: no such file or directory\n"
     ]
    }
   ],
   "source": [
    "# install dependencies\n",
    "\n",
    "! pip install -q joblib  # joblib for memoizing functions\n",
    "! pip install -q ipywidgets widgetsnbextension pandas-profiling # IProgress for progress bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add model_merging to the python path\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "model_merging_base = os.path.abspath(\"../model_merging/\")\n",
    "# assert it exist\n",
    "assert os.path.exists(model_merging_base)\n",
    "if model_merging_base not in sys.path:\n",
    "    sys.path.append(model_merging_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import joblib for caching and distributed computing\n",
    "from math import sqrt\n",
    "\n",
    "from joblib import Memory, Parallel, delayed\n",
    "\n",
    "# memory = Memory(location=\"cache\", verbose=10)\n",
    "memory = Memory(location=\"cache\", verbose=0)\n",
    "\n",
    "parallel = Parallel(n_jobs=2, return_as=\"generator\")\n",
    "output_generator = parallel(delayed(sqrt)(i**2) for i in range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and cached functions\n",
    "\n",
    "import os\n",
    "\n",
    "from llm_weaver import (\n",
    "    calculate_score_from_weaving_config,\n",
    "    get_score_from_named_model,\n",
    "    test_weaver,\n",
    ")\n",
    "\n",
    "# Disable parallelism in tokenizers to avoid deadlocks\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "calculate_score_from_weaving_config_cached = memory.cache(\n",
    "    calculate_score_from_weaving_config\n",
    ")\n",
    "test_weaver_cached = memory.cache(test_weaver)\n",
    "\n",
    "get_score_from_named_model_cached = memory.cache(get_score_from_named_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make sure you can build using `.build()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have transformers version 4.35.0!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-27 10:38:35.081384: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-27 10:38:35.081848: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "transformers.models.roberta.modeling_tf_roberta.TFRobertaForSequenceClassification"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "from llm_weaver import get_blank_model, get_model_config\n",
    "\n",
    "if transformers.__version__ < \"4.3.1\":\n",
    "    raise ValueError(\n",
    "        \"Need transformers >= 4.3.1, or something like that. Not sure of the version.\"\n",
    "    )\n",
    "    # https://github.com/huggingface/transformers/commit/4a55e4787760fdb6c40a972a60d814ba05425da1#diff-648ec06beb5ae6380c7f611a0f513a5d392509497d245a09f06b6549358afdffR1151\n",
    "\n",
    "print(f\"You have transformers version {transformers.__version__}!\")\n",
    "\n",
    "model = get_blank_model(get_model_config(\"textattack/roberta-base-RTE\"))\n",
    "model.build()\n",
    "\n",
    "type(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Get cross-task scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'textattack/roberta-base-RTE': ['textattack/roberta-base-RTE',\n",
       "  'textattack/distilbert-base-uncased-RTE'],\n",
       " 'textattack/distilbert-base-uncased-RTE': ['textattack/roberta-base-RTE',\n",
       "  'textattack/distilbert-base-uncased-RTE']}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ids = [\n",
    "    \"textattack/roberta-base-RTE\",\n",
    "    # \"textattack/bert-base-uncased-RTE\",\n",
    "    \"textattack/distilbert-base-uncased-RTE\"\n",
    "    # \"textattack/roberta-base-MNLI\",\n",
    "    # \"howey/roberta-large-rte\",\n",
    "    # \"howey/roberta-large-mnli\",\n",
    "    # \"JeremiahZ/roberta-base-rte\",\n",
    "    # \"JeremiahZ/roberta-base-mnli\",\n",
    "]\n",
    "\n",
    "model_ids_head_to_bodies = {head: model_ids for head in model_ids}\n",
    "model_ids_head_to_bodies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "You are using a model of type distilbert to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForSequenceClassification: ['distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.embeddings.word_embeddings.weight', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'pre_classifier.bias', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'classifier.bias', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'pre_classifier.weight', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'classifier.weight', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.3.attention.k_lin.bias']\n",
      "- This IS expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFRobertaForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'embeddings.word_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You are using a model of type distilbert to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForSequenceClassification: ['distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.embeddings.word_embeddings.weight', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'pre_classifier.bias', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'classifier.bias', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'pre_classifier.weight', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'classifier.weight', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.3.attention.k_lin.bias']\n",
      "- This IS expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFRobertaForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'embeddings.word_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "You are using a model of type distilbert to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForSequenceClassification: ['distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.embeddings.word_embeddings.weight', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'pre_classifier.bias', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'classifier.bias', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'pre_classifier.weight', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'classifier.weight', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.3.attention.k_lin.bias']\n",
      "- This IS expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFRobertaForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'embeddings.word_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You are using a model of type distilbert to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForSequenceClassification: ['distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.embeddings.word_embeddings.weight', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'pre_classifier.bias', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'classifier.bias', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'pre_classifier.weight', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'classifier.weight', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.3.attention.k_lin.bias']\n",
      "- This IS expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFRobertaForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'embeddings.word_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'glue_task': 'rte',\n",
       "  'tokenizer_model_id': 'textattack/roberta-base-RTE',\n",
       "  'blank_model_config': {'return_dict': True,\n",
       "   'output_hidden_states': False,\n",
       "   'output_attentions': False,\n",
       "   'torchscript': False,\n",
       "   'torch_dtype': None,\n",
       "   'use_bfloat16': False,\n",
       "   'tf_legacy_loss': False,\n",
       "   'pruned_heads': {},\n",
       "   'tie_word_embeddings': True,\n",
       "   'is_encoder_decoder': False,\n",
       "   'is_decoder': False,\n",
       "   'cross_attention_hidden_size': None,\n",
       "   'add_cross_attention': False,\n",
       "   'tie_encoder_decoder': False,\n",
       "   'max_length': 20,\n",
       "   'min_length': 0,\n",
       "   'do_sample': False,\n",
       "   'early_stopping': False,\n",
       "   'num_beams': 1,\n",
       "   'num_beam_groups': 1,\n",
       "   'diversity_penalty': 0.0,\n",
       "   'temperature': 1.0,\n",
       "   'top_k': 50,\n",
       "   'top_p': 1.0,\n",
       "   'typical_p': 1.0,\n",
       "   'repetition_penalty': 1.0,\n",
       "   'length_penalty': 1.0,\n",
       "   'no_repeat_ngram_size': 0,\n",
       "   'encoder_no_repeat_ngram_size': 0,\n",
       "   'bad_words_ids': None,\n",
       "   'num_return_sequences': 1,\n",
       "   'chunk_size_feed_forward': 0,\n",
       "   'output_scores': False,\n",
       "   'return_dict_in_generate': False,\n",
       "   'forced_bos_token_id': None,\n",
       "   'forced_eos_token_id': None,\n",
       "   'remove_invalid_values': False,\n",
       "   'exponential_decay_length_penalty': None,\n",
       "   'suppress_tokens': None,\n",
       "   'begin_suppress_tokens': None,\n",
       "   'architectures': ['RobertaForSequenceClassification'],\n",
       "   'finetuning_task': 'glue:rte',\n",
       "   'id2label': {0: 'LABEL_0', 1: 'LABEL_1'},\n",
       "   'label2id': {'LABEL_0': 0, 'LABEL_1': 1},\n",
       "   'tokenizer_class': None,\n",
       "   'prefix': None,\n",
       "   'bos_token_id': 0,\n",
       "   'pad_token_id': 1,\n",
       "   'eos_token_id': 2,\n",
       "   'sep_token_id': None,\n",
       "   'decoder_start_token_id': None,\n",
       "   'task_specific_params': None,\n",
       "   'problem_type': None,\n",
       "   'transformers_version': '4.35.0',\n",
       "   'gradient_checkpointing': False,\n",
       "   'model_type': 'roberta',\n",
       "   'vocab_size': 50265,\n",
       "   'hidden_size': 768,\n",
       "   'num_hidden_layers': 12,\n",
       "   'num_attention_heads': 12,\n",
       "   'hidden_act': 'gelu',\n",
       "   'intermediate_size': 3072,\n",
       "   'hidden_dropout_prob': 0.1,\n",
       "   'attention_probs_dropout_prob': 0.1,\n",
       "   'max_position_embeddings': 514,\n",
       "   'type_vocab_size': 1,\n",
       "   'initializer_range': 0.02,\n",
       "   'layer_norm_eps': 1e-05,\n",
       "   'position_embedding_type': 'absolute',\n",
       "   'use_cache': True,\n",
       "   'classifier_dropout': None},\n",
       "  'layer_assignments': [{'type': 'SingleLayer',\n",
       "    'params': {'donor': 'textattack/roberta-base-RTE',\n",
       "     'hidden_layer_number': 0}},\n",
       "   {'type': 'SingleLayer',\n",
       "    'params': {'donor': 'textattack/roberta-base-RTE',\n",
       "     'hidden_layer_number': 1}},\n",
       "   {'type': 'SingleLayer',\n",
       "    'params': {'donor': 'textattack/roberta-base-RTE',\n",
       "     'hidden_layer_number': 2}},\n",
       "   {'type': 'SingleLayer',\n",
       "    'params': {'donor': 'textattack/roberta-base-RTE',\n",
       "     'hidden_layer_number': 3}},\n",
       "   {'type': 'SingleLayer',\n",
       "    'params': {'donor': 'textattack/roberta-base-RTE',\n",
       "     'hidden_layer_number': 4}},\n",
       "   {'type': 'SingleLayer',\n",
       "    'params': {'donor': 'textattack/roberta-base-RTE',\n",
       "     'hidden_layer_number': 5}},\n",
       "   {'type': 'SingleLayer',\n",
       "    'params': {'donor': 'textattack/roberta-base-RTE',\n",
       "     'hidden_layer_number': 6}},\n",
       "   {'type': 'SingleLayer',\n",
       "    'params': {'donor': 'textattack/roberta-base-RTE',\n",
       "     'hidden_layer_number': 7}},\n",
       "   {'type': 'SingleLayer',\n",
       "    'params': {'donor': 'textattack/roberta-base-RTE',\n",
       "     'hidden_layer_number': 8}},\n",
       "   {'type': 'SingleLayer',\n",
       "    'params': {'donor': 'textattack/roberta-base-RTE',\n",
       "     'hidden_layer_number': 9}},\n",
       "   {'type': 'SingleLayer',\n",
       "    'params': {'donor': 'textattack/roberta-base-RTE',\n",
       "     'hidden_layer_number': 10}},\n",
       "   {'type': 'SingleLayer',\n",
       "    'params': {'donor': 'textattack/roberta-base-RTE',\n",
       "     'hidden_layer_number': 11}}],\n",
       "  'classification_head': {'type': 'SingleClassificationHead',\n",
       "   'params': {'donor': 'textattack/roberta-base-RTE'}},\n",
       "  'embeddings': {'type': 'SingleEmbeddings',\n",
       "   'params': {'donor': 'textattack/roberta-base-RTE'}}},\n",
       " {'glue_task': 'rte',\n",
       "  'tokenizer_model_id': 'textattack/roberta-base-RTE',\n",
       "  'blank_model_config': {'return_dict': True,\n",
       "   'output_hidden_states': False,\n",
       "   'output_attentions': False,\n",
       "   'torchscript': False,\n",
       "   'torch_dtype': None,\n",
       "   'use_bfloat16': False,\n",
       "   'tf_legacy_loss': False,\n",
       "   'pruned_heads': {},\n",
       "   'tie_word_embeddings': True,\n",
       "   'is_encoder_decoder': False,\n",
       "   'is_decoder': False,\n",
       "   'cross_attention_hidden_size': None,\n",
       "   'add_cross_attention': False,\n",
       "   'tie_encoder_decoder': False,\n",
       "   'max_length': 20,\n",
       "   'min_length': 0,\n",
       "   'do_sample': False,\n",
       "   'early_stopping': False,\n",
       "   'num_beams': 1,\n",
       "   'num_beam_groups': 1,\n",
       "   'diversity_penalty': 0.0,\n",
       "   'temperature': 1.0,\n",
       "   'top_k': 50,\n",
       "   'top_p': 1.0,\n",
       "   'typical_p': 1.0,\n",
       "   'repetition_penalty': 1.0,\n",
       "   'length_penalty': 1.0,\n",
       "   'no_repeat_ngram_size': 0,\n",
       "   'encoder_no_repeat_ngram_size': 0,\n",
       "   'bad_words_ids': None,\n",
       "   'num_return_sequences': 1,\n",
       "   'chunk_size_feed_forward': 0,\n",
       "   'output_scores': False,\n",
       "   'return_dict_in_generate': False,\n",
       "   'forced_bos_token_id': None,\n",
       "   'forced_eos_token_id': None,\n",
       "   'remove_invalid_values': False,\n",
       "   'exponential_decay_length_penalty': None,\n",
       "   'suppress_tokens': None,\n",
       "   'begin_suppress_tokens': None,\n",
       "   'architectures': ['RobertaForSequenceClassification'],\n",
       "   'finetuning_task': 'glue:rte',\n",
       "   'id2label': {0: 'LABEL_0', 1: 'LABEL_1'},\n",
       "   'label2id': {'LABEL_0': 0, 'LABEL_1': 1},\n",
       "   'tokenizer_class': None,\n",
       "   'prefix': None,\n",
       "   'bos_token_id': 0,\n",
       "   'pad_token_id': 1,\n",
       "   'eos_token_id': 2,\n",
       "   'sep_token_id': None,\n",
       "   'decoder_start_token_id': None,\n",
       "   'task_specific_params': None,\n",
       "   'problem_type': None,\n",
       "   'transformers_version': '4.35.0',\n",
       "   'gradient_checkpointing': False,\n",
       "   'model_type': 'roberta',\n",
       "   'vocab_size': 50265,\n",
       "   'hidden_size': 768,\n",
       "   'num_hidden_layers': 12,\n",
       "   'num_attention_heads': 12,\n",
       "   'hidden_act': 'gelu',\n",
       "   'intermediate_size': 3072,\n",
       "   'hidden_dropout_prob': 0.1,\n",
       "   'attention_probs_dropout_prob': 0.1,\n",
       "   'max_position_embeddings': 514,\n",
       "   'type_vocab_size': 1,\n",
       "   'initializer_range': 0.02,\n",
       "   'layer_norm_eps': 1e-05,\n",
       "   'position_embedding_type': 'absolute',\n",
       "   'use_cache': True,\n",
       "   'classifier_dropout': None},\n",
       "  'layer_assignments': [{'type': 'SingleLayer',\n",
       "    'params': {'donor': 'textattack/distilbert-base-uncased-RTE',\n",
       "     'hidden_layer_number': 0}},\n",
       "   {'type': 'SingleLayer',\n",
       "    'params': {'donor': 'textattack/distilbert-base-uncased-RTE',\n",
       "     'hidden_layer_number': 1}},\n",
       "   {'type': 'SingleLayer',\n",
       "    'params': {'donor': 'textattack/distilbert-base-uncased-RTE',\n",
       "     'hidden_layer_number': 2}},\n",
       "   {'type': 'SingleLayer',\n",
       "    'params': {'donor': 'textattack/distilbert-base-uncased-RTE',\n",
       "     'hidden_layer_number': 3}},\n",
       "   {'type': 'SingleLayer',\n",
       "    'params': {'donor': 'textattack/distilbert-base-uncased-RTE',\n",
       "     'hidden_layer_number': 4}},\n",
       "   {'type': 'SingleLayer',\n",
       "    'params': {'donor': 'textattack/distilbert-base-uncased-RTE',\n",
       "     'hidden_layer_number': 5}},\n",
       "   {'type': 'SingleLayer',\n",
       "    'params': {'donor': 'textattack/distilbert-base-uncased-RTE',\n",
       "     'hidden_layer_number': 6}},\n",
       "   {'type': 'SingleLayer',\n",
       "    'params': {'donor': 'textattack/distilbert-base-uncased-RTE',\n",
       "     'hidden_layer_number': 7}},\n",
       "   {'type': 'SingleLayer',\n",
       "    'params': {'donor': 'textattack/distilbert-base-uncased-RTE',\n",
       "     'hidden_layer_number': 8}},\n",
       "   {'type': 'SingleLayer',\n",
       "    'params': {'donor': 'textattack/distilbert-base-uncased-RTE',\n",
       "     'hidden_layer_number': 9}},\n",
       "   {'type': 'SingleLayer',\n",
       "    'params': {'donor': 'textattack/distilbert-base-uncased-RTE',\n",
       "     'hidden_layer_number': 10}},\n",
       "   {'type': 'SingleLayer',\n",
       "    'params': {'donor': 'textattack/distilbert-base-uncased-RTE',\n",
       "     'hidden_layer_number': 11}}],\n",
       "  'classification_head': {'type': 'SingleClassificationHead',\n",
       "   'params': {'donor': 'textattack/roberta-base-RTE'}},\n",
       "  'embeddings': {'type': 'SingleEmbeddings',\n",
       "   'params': {'donor': 'textattack/roberta-base-RTE'}}},\n",
       " {'glue_task': 'rte',\n",
       "  'tokenizer_model_id': 'textattack/distilbert-base-uncased-RTE',\n",
       "  'blank_model_config': {'return_dict': True,\n",
       "   'output_hidden_states': False,\n",
       "   'output_attentions': False,\n",
       "   'torchscript': False,\n",
       "   'torch_dtype': None,\n",
       "   'use_bfloat16': False,\n",
       "   'tf_legacy_loss': False,\n",
       "   'pruned_heads': {},\n",
       "   'tie_word_embeddings': True,\n",
       "   'is_encoder_decoder': False,\n",
       "   'is_decoder': False,\n",
       "   'cross_attention_hidden_size': None,\n",
       "   'add_cross_attention': False,\n",
       "   'tie_encoder_decoder': False,\n",
       "   'max_length': 20,\n",
       "   'min_length': 0,\n",
       "   'do_sample': False,\n",
       "   'early_stopping': False,\n",
       "   'num_beams': 1,\n",
       "   'num_beam_groups': 1,\n",
       "   'diversity_penalty': 0.0,\n",
       "   'temperature': 1.0,\n",
       "   'top_k': 50,\n",
       "   'top_p': 1.0,\n",
       "   'typical_p': 1.0,\n",
       "   'repetition_penalty': 1.0,\n",
       "   'length_penalty': 1.0,\n",
       "   'no_repeat_ngram_size': 0,\n",
       "   'encoder_no_repeat_ngram_size': 0,\n",
       "   'bad_words_ids': None,\n",
       "   'num_return_sequences': 1,\n",
       "   'chunk_size_feed_forward': 0,\n",
       "   'output_scores': False,\n",
       "   'return_dict_in_generate': False,\n",
       "   'forced_bos_token_id': None,\n",
       "   'forced_eos_token_id': None,\n",
       "   'remove_invalid_values': False,\n",
       "   'exponential_decay_length_penalty': None,\n",
       "   'suppress_tokens': None,\n",
       "   'begin_suppress_tokens': None,\n",
       "   'architectures': ['DistilBertForSequenceClassification'],\n",
       "   'finetuning_task': 'glue:rte',\n",
       "   'id2label': {0: 'LABEL_0', 1: 'LABEL_1'},\n",
       "   'label2id': {'LABEL_0': 0, 'LABEL_1': 1},\n",
       "   'tokenizer_class': None,\n",
       "   'prefix': None,\n",
       "   'bos_token_id': 0,\n",
       "   'pad_token_id': 0,\n",
       "   'eos_token_id': 2,\n",
       "   'sep_token_id': None,\n",
       "   'decoder_start_token_id': None,\n",
       "   'task_specific_params': None,\n",
       "   'problem_type': None,\n",
       "   'transformers_version': '4.35.0',\n",
       "   'activation': 'gelu',\n",
       "   'attention_dropout': 0.1,\n",
       "   'dim': 768,\n",
       "   'dropout': 0.1,\n",
       "   'hidden_dim': 3072,\n",
       "   'model_type': 'roberta',\n",
       "   'n_heads': 12,\n",
       "   'n_layers': 6,\n",
       "   'qa_dropout': 0.1,\n",
       "   'seq_classif_dropout': 0.2,\n",
       "   'sinusoidal_pos_embds': False,\n",
       "   'tie_weights_': True,\n",
       "   'vocab_size': 30522,\n",
       "   'hidden_size': 768,\n",
       "   'num_hidden_layers': 12,\n",
       "   'num_attention_heads': 12,\n",
       "   'hidden_act': 'gelu',\n",
       "   'intermediate_size': 3072,\n",
       "   'hidden_dropout_prob': 0.1,\n",
       "   'attention_probs_dropout_prob': 0.1,\n",
       "   'max_position_embeddings': 512,\n",
       "   'type_vocab_size': 2,\n",
       "   'initializer_range': 0.02,\n",
       "   'layer_norm_eps': 1e-12,\n",
       "   'position_embedding_type': 'absolute',\n",
       "   'use_cache': True,\n",
       "   'classifier_dropout': None},\n",
       "  'layer_assignments': [{'type': 'SingleLayer',\n",
       "    'params': {'donor': 'textattack/roberta-base-RTE',\n",
       "     'hidden_layer_number': 0}},\n",
       "   {'type': 'SingleLayer',\n",
       "    'params': {'donor': 'textattack/roberta-base-RTE',\n",
       "     'hidden_layer_number': 1}},\n",
       "   {'type': 'SingleLayer',\n",
       "    'params': {'donor': 'textattack/roberta-base-RTE',\n",
       "     'hidden_layer_number': 2}},\n",
       "   {'type': 'SingleLayer',\n",
       "    'params': {'donor': 'textattack/roberta-base-RTE',\n",
       "     'hidden_layer_number': 3}},\n",
       "   {'type': 'SingleLayer',\n",
       "    'params': {'donor': 'textattack/roberta-base-RTE',\n",
       "     'hidden_layer_number': 4}},\n",
       "   {'type': 'SingleLayer',\n",
       "    'params': {'donor': 'textattack/roberta-base-RTE',\n",
       "     'hidden_layer_number': 5}},\n",
       "   {'type': 'SingleLayer',\n",
       "    'params': {'donor': 'textattack/roberta-base-RTE',\n",
       "     'hidden_layer_number': 6}},\n",
       "   {'type': 'SingleLayer',\n",
       "    'params': {'donor': 'textattack/roberta-base-RTE',\n",
       "     'hidden_layer_number': 7}},\n",
       "   {'type': 'SingleLayer',\n",
       "    'params': {'donor': 'textattack/roberta-base-RTE',\n",
       "     'hidden_layer_number': 8}},\n",
       "   {'type': 'SingleLayer',\n",
       "    'params': {'donor': 'textattack/roberta-base-RTE',\n",
       "     'hidden_layer_number': 9}},\n",
       "   {'type': 'SingleLayer',\n",
       "    'params': {'donor': 'textattack/roberta-base-RTE',\n",
       "     'hidden_layer_number': 10}},\n",
       "   {'type': 'SingleLayer',\n",
       "    'params': {'donor': 'textattack/roberta-base-RTE',\n",
       "     'hidden_layer_number': 11}}],\n",
       "  'classification_head': {'type': 'SingleClassificationHead',\n",
       "   'params': {'donor': 'textattack/distilbert-base-uncased-RTE'}},\n",
       "  'embeddings': {'type': 'SingleEmbeddings',\n",
       "   'params': {'donor': 'textattack/distilbert-base-uncased-RTE'}}},\n",
       " {'glue_task': 'rte',\n",
       "  'tokenizer_model_id': 'textattack/distilbert-base-uncased-RTE',\n",
       "  'blank_model_config': {'return_dict': True,\n",
       "   'output_hidden_states': False,\n",
       "   'output_attentions': False,\n",
       "   'torchscript': False,\n",
       "   'torch_dtype': None,\n",
       "   'use_bfloat16': False,\n",
       "   'tf_legacy_loss': False,\n",
       "   'pruned_heads': {},\n",
       "   'tie_word_embeddings': True,\n",
       "   'is_encoder_decoder': False,\n",
       "   'is_decoder': False,\n",
       "   'cross_attention_hidden_size': None,\n",
       "   'add_cross_attention': False,\n",
       "   'tie_encoder_decoder': False,\n",
       "   'max_length': 20,\n",
       "   'min_length': 0,\n",
       "   'do_sample': False,\n",
       "   'early_stopping': False,\n",
       "   'num_beams': 1,\n",
       "   'num_beam_groups': 1,\n",
       "   'diversity_penalty': 0.0,\n",
       "   'temperature': 1.0,\n",
       "   'top_k': 50,\n",
       "   'top_p': 1.0,\n",
       "   'typical_p': 1.0,\n",
       "   'repetition_penalty': 1.0,\n",
       "   'length_penalty': 1.0,\n",
       "   'no_repeat_ngram_size': 0,\n",
       "   'encoder_no_repeat_ngram_size': 0,\n",
       "   'bad_words_ids': None,\n",
       "   'num_return_sequences': 1,\n",
       "   'chunk_size_feed_forward': 0,\n",
       "   'output_scores': False,\n",
       "   'return_dict_in_generate': False,\n",
       "   'forced_bos_token_id': None,\n",
       "   'forced_eos_token_id': None,\n",
       "   'remove_invalid_values': False,\n",
       "   'exponential_decay_length_penalty': None,\n",
       "   'suppress_tokens': None,\n",
       "   'begin_suppress_tokens': None,\n",
       "   'architectures': ['DistilBertForSequenceClassification'],\n",
       "   'finetuning_task': 'glue:rte',\n",
       "   'id2label': {0: 'LABEL_0', 1: 'LABEL_1'},\n",
       "   'label2id': {'LABEL_0': 0, 'LABEL_1': 1},\n",
       "   'tokenizer_class': None,\n",
       "   'prefix': None,\n",
       "   'bos_token_id': 0,\n",
       "   'pad_token_id': 0,\n",
       "   'eos_token_id': 2,\n",
       "   'sep_token_id': None,\n",
       "   'decoder_start_token_id': None,\n",
       "   'task_specific_params': None,\n",
       "   'problem_type': None,\n",
       "   'transformers_version': '4.35.0',\n",
       "   'activation': 'gelu',\n",
       "   'attention_dropout': 0.1,\n",
       "   'dim': 768,\n",
       "   'dropout': 0.1,\n",
       "   'hidden_dim': 3072,\n",
       "   'model_type': 'roberta',\n",
       "   'n_heads': 12,\n",
       "   'n_layers': 6,\n",
       "   'qa_dropout': 0.1,\n",
       "   'seq_classif_dropout': 0.2,\n",
       "   'sinusoidal_pos_embds': False,\n",
       "   'tie_weights_': True,\n",
       "   'vocab_size': 30522,\n",
       "   'hidden_size': 768,\n",
       "   'num_hidden_layers': 12,\n",
       "   'num_attention_heads': 12,\n",
       "   'hidden_act': 'gelu',\n",
       "   'intermediate_size': 3072,\n",
       "   'hidden_dropout_prob': 0.1,\n",
       "   'attention_probs_dropout_prob': 0.1,\n",
       "   'max_position_embeddings': 512,\n",
       "   'type_vocab_size': 2,\n",
       "   'initializer_range': 0.02,\n",
       "   'layer_norm_eps': 1e-12,\n",
       "   'position_embedding_type': 'absolute',\n",
       "   'use_cache': True,\n",
       "   'classifier_dropout': None},\n",
       "  'layer_assignments': [{'type': 'SingleLayer',\n",
       "    'params': {'donor': 'textattack/distilbert-base-uncased-RTE',\n",
       "     'hidden_layer_number': 0}},\n",
       "   {'type': 'SingleLayer',\n",
       "    'params': {'donor': 'textattack/distilbert-base-uncased-RTE',\n",
       "     'hidden_layer_number': 1}},\n",
       "   {'type': 'SingleLayer',\n",
       "    'params': {'donor': 'textattack/distilbert-base-uncased-RTE',\n",
       "     'hidden_layer_number': 2}},\n",
       "   {'type': 'SingleLayer',\n",
       "    'params': {'donor': 'textattack/distilbert-base-uncased-RTE',\n",
       "     'hidden_layer_number': 3}},\n",
       "   {'type': 'SingleLayer',\n",
       "    'params': {'donor': 'textattack/distilbert-base-uncased-RTE',\n",
       "     'hidden_layer_number': 4}},\n",
       "   {'type': 'SingleLayer',\n",
       "    'params': {'donor': 'textattack/distilbert-base-uncased-RTE',\n",
       "     'hidden_layer_number': 5}},\n",
       "   {'type': 'SingleLayer',\n",
       "    'params': {'donor': 'textattack/distilbert-base-uncased-RTE',\n",
       "     'hidden_layer_number': 6}},\n",
       "   {'type': 'SingleLayer',\n",
       "    'params': {'donor': 'textattack/distilbert-base-uncased-RTE',\n",
       "     'hidden_layer_number': 7}},\n",
       "   {'type': 'SingleLayer',\n",
       "    'params': {'donor': 'textattack/distilbert-base-uncased-RTE',\n",
       "     'hidden_layer_number': 8}},\n",
       "   {'type': 'SingleLayer',\n",
       "    'params': {'donor': 'textattack/distilbert-base-uncased-RTE',\n",
       "     'hidden_layer_number': 9}},\n",
       "   {'type': 'SingleLayer',\n",
       "    'params': {'donor': 'textattack/distilbert-base-uncased-RTE',\n",
       "     'hidden_layer_number': 10}},\n",
       "   {'type': 'SingleLayer',\n",
       "    'params': {'donor': 'textattack/distilbert-base-uncased-RTE',\n",
       "     'hidden_layer_number': 11}}],\n",
       "  'classification_head': {'type': 'SingleClassificationHead',\n",
       "   'params': {'donor': 'textattack/distilbert-base-uncased-RTE'}},\n",
       "  'embeddings': {'type': 'SingleEmbeddings',\n",
       "   'params': {'donor': 'textattack/distilbert-base-uncased-RTE'}}}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from llm_weaver import dict_overwrite, get_model_config, normalize_glue_task_name\n",
    "\n",
    "from model_merging import hdf5_util, sample_layers\n",
    "\n",
    "\n",
    "def multi_task_configs_iter(model_ids_head_to_bodies, max_configs=None):\n",
    "    num_configs = 0\n",
    "    for head_model_id, body_model_ids in model_ids_head_to_bodies.items():\n",
    "        for body_model_id in body_model_ids:\n",
    "            # Use the task model as the \"blank model\"\n",
    "            task = normalize_glue_task_name(head_model_id)\n",
    "            head_model_config = get_model_config(head_model_id)\n",
    "            body_model_config = get_model_config(body_model_id)\n",
    "            config = {\n",
    "                \"glue_task\": task,\n",
    "                \"tokenizer_model_id\": head_model_id,\n",
    "                # The task (i.e. the classification head output size should match the task at hand)\n",
    "                \"blank_model_config\": dict_overwrite(\n",
    "                    head_model_config,\n",
    "                    {\"num_hidden_layers\": body_model_config[\"num_hidden_layers\"]},\n",
    "                ),\n",
    "                # Layer assignments\n",
    "                \"layer_assignments\": [\n",
    "                    {\n",
    "                        \"type\": \"SingleLayer\",\n",
    "                        \"params\": {\n",
    "                            \"donor\": body_model_id,\n",
    "                            \"hidden_layer_number\": i,\n",
    "                        },\n",
    "                    }\n",
    "                    for i in range(body_model_config[\"num_hidden_layers\"])\n",
    "                ],\n",
    "                # The head (i.e. the classification head should match the task at hand)\n",
    "                # THESE ARE DIFFERENT BETWEEN RTE AND MNLI\n",
    "                \"classification_head\": {\n",
    "                    \"type\": \"SingleClassificationHead\",\n",
    "                    \"params\": {\n",
    "                        \"donor\": head_model_id,\n",
    "                    },\n",
    "                },\n",
    "                # The embeddings layer\n",
    "                # THESE ARE DIFFERENT BETWEEN RTE AND MNLI\n",
    "                \"embeddings\": {\n",
    "                    \"type\": \"SingleEmbeddings\",\n",
    "                    \"params\": {\n",
    "                        # \"donor\": body_model_id,\n",
    "                        \"donor\": head_model_id,\n",
    "                    },\n",
    "                },\n",
    "            }\n",
    "            num_configs += 1\n",
    "            if max_configs and num_configs > max_configs:\n",
    "                break\n",
    "            yield config\n",
    "\n",
    "\n",
    "list(\n",
    "    multi_task_configs_iter(\n",
    "        model_ids_head_to_bodies=model_ids_head_to_bodies,\n",
    "        max_configs=4,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step get original model baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "You are using a model of type distilbert to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForSequenceClassification: ['distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.embeddings.word_embeddings.weight', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'pre_classifier.bias', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'classifier.bias', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'pre_classifier.weight', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'classifier.weight', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.3.attention.k_lin.bias']\n",
      "- This IS expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFRobertaForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'embeddings.word_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating score for weaving config md5sum: c523724abaf659994a2680f089e4610a\n",
      "Loading textattack/roberta-base-RTE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "You are using a model of type distilbert to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading textattack/distilbert-base-uncased-RTE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForSequenceClassification: ['distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.embeddings.word_embeddings.weight', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'pre_classifier.bias', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'classifier.bias', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'pre_classifier.weight', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'classifier.weight', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.3.attention.k_lin.bias']\n",
      "- This IS expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFRobertaForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'embeddings.word_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._0/attention/self/query/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._0/attention/self/key/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._0/attention/self/value/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._0/attention/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._0/attention/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._0/intermediate/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._0/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._0/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._1/attention/self/query/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._1/attention/self/key/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._1/attention/self/value/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._1/attention/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._1/attention/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._1/intermediate/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._1/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._1/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._2/attention/self/query/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._2/attention/self/key/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._2/attention/self/value/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._2/attention/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._2/attention/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._2/intermediate/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._2/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._2/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._3/attention/self/query/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._3/attention/self/key/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._3/attention/self/value/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._3/attention/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._3/attention/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._3/intermediate/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._3/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._3/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._4/attention/self/query/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._4/attention/self/key/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._4/attention/self/value/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._4/attention/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._4/attention/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._4/intermediate/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._4/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._4/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._5/attention/self/query/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._5/attention/self/key/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._5/attention/self/value/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._5/attention/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._5/attention/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._5/intermediate/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._5/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._5/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._6/attention/self/query/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._6/attention/self/key/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._6/attention/self/value/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._6/attention/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._6/attention/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._6/intermediate/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._6/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._6/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._7/attention/self/query/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._7/attention/self/key/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._7/attention/self/value/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._7/attention/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._7/attention/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._7/intermediate/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._7/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._7/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._8/attention/self/query/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._8/attention/self/key/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._8/attention/self/value/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._8/attention/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._8/attention/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._8/intermediate/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._8/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._8/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._9/attention/self/query/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._9/attention/self/key/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._9/attention/self/value/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._9/attention/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._9/attention/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._9/intermediate/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._9/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._9/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._10/attention/self/query/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._10/attention/self/key/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._10/attention/self/value/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._10/attention/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._10/attention/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._10/intermediate/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._10/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._10/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._11/attention/self/query/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._11/attention/self/key/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._11/attention/self/value/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._11/attention/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._11/attention/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._11/intermediate/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._11/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._11/output/LayerNorm/beta:0 is still zero (0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brian/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the  Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "/home/brian/2023-fall-cs-194-294-merging-llms/model_merging/model_merging/evaluation.py:7: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library  Evaluate: https://huggingface.co/docs/evaluate\n",
      "  return hfds.load_metric(\"glue\", task)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._0/attention/self/query/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._0/attention/self/key/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._0/attention/self/value/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._0/attention/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._0/attention/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._0/intermediate/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._0/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._0/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._1/attention/self/query/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._1/attention/self/key/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._1/attention/self/value/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._1/attention/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._1/attention/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._1/intermediate/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._1/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._1/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._2/attention/self/query/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._2/attention/self/key/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._2/attention/self/value/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._2/attention/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._2/attention/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._2/intermediate/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._2/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._2/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._3/attention/self/query/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._3/attention/self/key/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._3/attention/self/value/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._3/attention/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._3/attention/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._3/intermediate/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._3/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._3/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._4/attention/self/query/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._4/attention/self/key/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._4/attention/self/value/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._4/attention/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._4/attention/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._4/intermediate/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._4/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._4/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._5/attention/self/query/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._5/attention/self/key/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._5/attention/self/value/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._5/attention/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._5/attention/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._5/intermediate/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._5/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._5/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._6/attention/self/query/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._6/attention/self/key/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._6/attention/self/value/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._6/attention/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._6/attention/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._6/intermediate/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._6/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._6/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._7/attention/self/query/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._7/attention/self/key/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._7/attention/self/value/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._7/attention/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._7/attention/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._7/intermediate/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._7/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._7/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._8/attention/self/query/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._8/attention/self/key/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._8/attention/self/value/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._8/attention/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._8/attention/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._8/intermediate/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._8/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._8/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._9/attention/self/query/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._9/attention/self/key/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._9/attention/self/value/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._9/attention/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._9/attention/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._9/intermediate/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._9/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._9/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._10/attention/self/query/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._10/attention/self/key/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._10/attention/self/value/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._10/attention/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._10/attention/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._10/intermediate/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._10/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._10/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._11/attention/self/query/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._11/attention/self/key/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._11/attention/self/value/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._11/attention/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._11/attention/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._11/intermediate/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._11/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_14/roberta/encoder/layer_._11/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: All predictions are the same! Is your model broken? [0]\n",
      "WARNING: All predictions are the same! Is your model broken? [0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-27 10:39:29.269835: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "You are using a model of type distilbert to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: All predictions are the same! Is your model broken? [0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForSequenceClassification: ['distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.embeddings.word_embeddings.weight', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'pre_classifier.bias', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'classifier.bias', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'pre_classifier.weight', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'classifier.weight', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.3.attention.k_lin.bias']\n",
      "- This IS expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFRobertaForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'embeddings.word_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating score for weaving config md5sum: 950868d95040810431f2959422e87d51\n",
      "Loading textattack/roberta-base-RTE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "You are using a model of type distilbert to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading textattack/distilbert-base-uncased-RTE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForSequenceClassification: ['distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.embeddings.word_embeddings.weight', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'pre_classifier.bias', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'classifier.bias', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'pre_classifier.weight', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'classifier.weight', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.3.attention.k_lin.bias']\n",
      "- This IS expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFRobertaForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'embeddings.word_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: tf_roberta_for_sequence_classification_19/roberta/embeddings/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_19/classifier/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_19/classifier/out_proj/bias:0 is still zero (0.0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdc71d7de1414f3cb879e3de1278f8a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fac553d5c4349e1a23d6dc514efbbe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dde7ceb175394726a0d2dbf5299e3f01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading ()cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brian/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the  Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: tf_roberta_for_sequence_classification_19/roberta/embeddings/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_19/classifier/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_19/classifier/out_proj/bias:0 is still zero (0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-27 10:39:59.854676: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "You are using a model of type distilbert to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForSequenceClassification: ['distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.embeddings.word_embeddings.weight', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'pre_classifier.bias', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'classifier.bias', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'pre_classifier.weight', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'classifier.weight', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.3.attention.k_lin.bias']\n",
      "- This IS expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFRobertaForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'embeddings.word_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You are using a model of type distilbert to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForSequenceClassification: ['distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.embeddings.word_embeddings.weight', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'pre_classifier.bias', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'classifier.bias', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'pre_classifier.weight', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'classifier.weight', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.3.attention.k_lin.bias']\n",
      "- This IS expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFRobertaForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'embeddings.word_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating score for weaving config md5sum: c0f1099f5513b5e5324caac782631618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type distilbert to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading textattack/distilbert-base-uncased-RTE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForSequenceClassification: ['distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.embeddings.word_embeddings.weight', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'pre_classifier.bias', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'classifier.bias', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'pre_classifier.weight', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'classifier.weight', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.3.attention.k_lin.bias']\n",
      "- This IS expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFRobertaForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'embeddings.word_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._0/attention/self/query/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._0/attention/self/key/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._0/attention/self/value/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._0/attention/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._0/attention/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._0/intermediate/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._0/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._0/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._1/attention/self/query/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._1/attention/self/key/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._1/attention/self/value/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._1/attention/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._1/attention/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._1/intermediate/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._1/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._1/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._2/attention/self/query/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._2/attention/self/key/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._2/attention/self/value/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._2/attention/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._2/attention/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._2/intermediate/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._2/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._2/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._3/attention/self/query/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._3/attention/self/key/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._3/attention/self/value/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._3/attention/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._3/attention/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._3/intermediate/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._3/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._3/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._4/attention/self/query/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._4/attention/self/key/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._4/attention/self/value/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._4/attention/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._4/attention/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._4/intermediate/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._4/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._4/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._5/attention/self/query/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._5/attention/self/key/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._5/attention/self/value/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._5/attention/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._5/attention/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._5/intermediate/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._5/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._5/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._6/attention/self/query/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._6/attention/self/key/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._6/attention/self/value/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._6/attention/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._6/attention/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._6/intermediate/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._6/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._6/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._7/attention/self/query/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._7/attention/self/key/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._7/attention/self/value/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._7/attention/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._7/attention/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._7/intermediate/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._7/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._7/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._8/attention/self/query/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._8/attention/self/key/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._8/attention/self/value/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._8/attention/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._8/attention/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._8/intermediate/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._8/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._8/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._9/attention/self/query/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._9/attention/self/key/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._9/attention/self/value/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._9/attention/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._9/attention/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._9/intermediate/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._9/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._9/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._10/attention/self/query/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._10/attention/self/key/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._10/attention/self/value/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._10/attention/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._10/attention/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._10/intermediate/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._10/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._10/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._11/attention/self/query/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._11/attention/self/key/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._11/attention/self/value/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._11/attention/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._11/attention/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._11/intermediate/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._11/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._11/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/embeddings/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/classifier/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/classifier/out_proj/bias:0 is still zero (0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brian/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the  Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._0/attention/self/query/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._0/attention/self/key/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._0/attention/self/value/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._0/attention/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._0/attention/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._0/intermediate/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._0/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._0/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._1/attention/self/query/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._1/attention/self/key/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._1/attention/self/value/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._1/attention/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._1/attention/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._1/intermediate/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._1/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._1/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._2/attention/self/query/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._2/attention/self/key/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._2/attention/self/value/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._2/attention/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._2/attention/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._2/intermediate/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._2/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._2/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._3/attention/self/query/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._3/attention/self/key/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._3/attention/self/value/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._3/attention/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._3/attention/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._3/intermediate/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._3/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._3/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._4/attention/self/query/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._4/attention/self/key/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._4/attention/self/value/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._4/attention/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._4/attention/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._4/intermediate/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._4/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._4/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._5/attention/self/query/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._5/attention/self/key/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._5/attention/self/value/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._5/attention/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._5/attention/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._5/intermediate/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._5/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._5/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._6/attention/self/query/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._6/attention/self/key/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._6/attention/self/value/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._6/attention/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._6/attention/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._6/intermediate/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._6/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._6/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._7/attention/self/query/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._7/attention/self/key/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._7/attention/self/value/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._7/attention/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._7/attention/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._7/intermediate/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._7/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._7/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._8/attention/self/query/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._8/attention/self/key/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._8/attention/self/value/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._8/attention/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._8/attention/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._8/intermediate/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._8/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._8/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._9/attention/self/query/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._9/attention/self/key/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._9/attention/self/value/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._9/attention/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._9/attention/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._9/intermediate/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._9/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._9/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._10/attention/self/query/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._10/attention/self/key/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._10/attention/self/value/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._10/attention/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._10/attention/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._10/intermediate/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._10/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._10/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._11/attention/self/query/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._11/attention/self/key/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._11/attention/self/value/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._11/attention/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._11/attention/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._11/intermediate/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._11/output/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/encoder/layer_._11/output/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/roberta/embeddings/LayerNorm/beta:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/classifier/dense/bias:0 is still zero (0.0)\n",
      "WARNING: tf_roberta_for_sequence_classification_24/classifier/out_proj/bias:0 is still zero (0.0)\n",
      "WARNING: All predictions are the same! Is your model broken? [0]\n",
      "WARNING: All predictions are the same! Is your model broken? [0]\n",
      "WARNING: All predictions are the same! Is your model broken? [0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-27 10:40:24.429033: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "4it [01:24, 21.23s/it]\n",
      "100%|| 1/1 [01:24<00:00, 84.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: All predictions are the same! Is your model broken? [0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task</th>\n",
       "      <th>classification_head_model</th>\n",
       "      <th>layers_models</th>\n",
       "      <th>split</th>\n",
       "      <th>n_examples</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rte</td>\n",
       "      <td>textattack/roberta-base-RTE</td>\n",
       "      <td>[textattack/roberta-base-RTE]</td>\n",
       "      <td>validation</td>\n",
       "      <td>256</td>\n",
       "      <td>0.726562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rte</td>\n",
       "      <td>textattack/roberta-base-RTE</td>\n",
       "      <td>[textattack/distilbert-base-uncased-RTE]</td>\n",
       "      <td>validation</td>\n",
       "      <td>256</td>\n",
       "      <td>0.519531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rte</td>\n",
       "      <td>textattack/distilbert-base-uncased-RTE</td>\n",
       "      <td>[textattack/roberta-base-RTE]</td>\n",
       "      <td>validation</td>\n",
       "      <td>256</td>\n",
       "      <td>0.476562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rte</td>\n",
       "      <td>textattack/distilbert-base-uncased-RTE</td>\n",
       "      <td>[textattack/distilbert-base-uncased-RTE]</td>\n",
       "      <td>validation</td>\n",
       "      <td>256</td>\n",
       "      <td>0.523438</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  task               classification_head_model  \\\n",
       "0  rte             textattack/roberta-base-RTE   \n",
       "1  rte             textattack/roberta-base-RTE   \n",
       "2  rte  textattack/distilbert-base-uncased-RTE   \n",
       "3  rte  textattack/distilbert-base-uncased-RTE   \n",
       "\n",
       "                              layers_models       split  n_examples  accuracy  \n",
       "0             [textattack/roberta-base-RTE]  validation         256  0.726562  \n",
       "1  [textattack/distilbert-base-uncased-RTE]  validation         256  0.519531  \n",
       "2             [textattack/roberta-base-RTE]  validation         256  0.476562  \n",
       "3  [textattack/distilbert-base-uncased-RTE]  validation         256  0.523438  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llm_weaver import normalize_glue_task_name\n",
    "from tqdm import tqdm\n",
    "\n",
    "n_examples = 256\n",
    "\n",
    "records = []\n",
    "for split in tqdm(\n",
    "    [\n",
    "        # \"train\",\n",
    "        \"validation\",\n",
    "        # \"test\"\n",
    "    ]\n",
    "):\n",
    "    for config in tqdm(\n",
    "        multi_task_configs_iter(\n",
    "            model_ids_head_to_bodies=model_ids_head_to_bodies,\n",
    "        )\n",
    "    ):\n",
    "        records.append(\n",
    "            {\n",
    "                \"task\": config[\"glue_task\"],\n",
    "                \"classification_head_model\": config[\"classification_head\"][\"params\"][\n",
    "                    \"donor\"\n",
    "                ],\n",
    "                \"layers_models\": list(\n",
    "                    sorted(\n",
    "                        set(\n",
    "                            [\n",
    "                                layer[\"params\"][\"donor\"]\n",
    "                                for layer in config[\"layer_assignments\"]\n",
    "                            ]\n",
    "                        )\n",
    "                    )\n",
    "                ),\n",
    "                \"score\": calculate_score_from_weaving_config_cached(\n",
    "                    weaving_config=config,\n",
    "                    split=split,\n",
    "                    n_examples=n_examples,\n",
    "                ),\n",
    "                \"split\": split,\n",
    "                \"n_examples\": n_examples,\n",
    "            }\n",
    "        )\n",
    "import pandas as pd\n",
    "\n",
    "# Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForSequenceClassification: ['roberta.embeddings.position_ids']\n",
    "df = pd.DataFrame.from_records(records)\n",
    "df = df.join(pd.json_normalize(df[\"score\"])).drop(columns=[\"score\"])\n",
    "# df[\"task\"] = df[\"model_id\"].apply(normalize_glue_task_name)\n",
    "# df[\"roberta\"] = df[\"model_id\"].apply(lambda x: \"large\" if \"large\" in x else \"base\")\n",
    "# df = df[df[\"split\"] == \"train\"]\n",
    "# df = df[~df[\"accuracy\"].isna()]\n",
    "# df = df.sort_values([\"task\", \"roberta\", \"split\"])\n",
    "# replace nan with ''\n",
    "df = df.fillna(\"\")\n",
    "# df.to_csv(\"test-weaving-on-base-models.original-scores.csv\", index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type distilbert to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForSequenceClassification: ['distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.embeddings.word_embeddings.weight', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'pre_classifier.weight', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'classifier.bias', 'pre_classifier.bias', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'classifier.weight', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.1.attention.v_lin.bias']\n",
      "- This IS expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFRobertaForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'embeddings.word_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._0/attention/self/query/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._0/attention/self/query/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._0/attention/self/key/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._0/attention/self/key/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._0/attention/self/value/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._0/attention/self/value/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._0/attention/output/dense/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._0/attention/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._0/attention/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._0/attention/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._0/intermediate/dense/kernel:0 (768, 3072)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._0/intermediate/dense/bias:0 (3072,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._0/output/dense/kernel:0 (3072, 768)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._0/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._0/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._0/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._1/attention/self/query/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._1/attention/self/query/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._1/attention/self/key/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._1/attention/self/key/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._1/attention/self/value/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._1/attention/self/value/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._1/attention/output/dense/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._1/attention/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._1/attention/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._1/attention/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._1/intermediate/dense/kernel:0 (768, 3072)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._1/intermediate/dense/bias:0 (3072,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._1/output/dense/kernel:0 (3072, 768)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._1/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._1/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._1/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._2/attention/self/query/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._2/attention/self/query/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._2/attention/self/key/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._2/attention/self/key/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._2/attention/self/value/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._2/attention/self/value/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._2/attention/output/dense/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._2/attention/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._2/attention/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._2/attention/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._2/intermediate/dense/kernel:0 (768, 3072)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._2/intermediate/dense/bias:0 (3072,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._2/output/dense/kernel:0 (3072, 768)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._2/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._2/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._2/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._3/attention/self/query/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._3/attention/self/query/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._3/attention/self/key/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._3/attention/self/key/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._3/attention/self/value/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._3/attention/self/value/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._3/attention/output/dense/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._3/attention/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._3/attention/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._3/attention/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._3/intermediate/dense/kernel:0 (768, 3072)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._3/intermediate/dense/bias:0 (3072,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._3/output/dense/kernel:0 (3072, 768)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._3/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._3/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._3/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._4/attention/self/query/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._4/attention/self/query/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._4/attention/self/key/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._4/attention/self/key/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._4/attention/self/value/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._4/attention/self/value/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._4/attention/output/dense/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._4/attention/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._4/attention/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._4/attention/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._4/intermediate/dense/kernel:0 (768, 3072)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._4/intermediate/dense/bias:0 (3072,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._4/output/dense/kernel:0 (3072, 768)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._4/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._4/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._4/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._5/attention/self/query/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._5/attention/self/query/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._5/attention/self/key/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._5/attention/self/key/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._5/attention/self/value/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._5/attention/self/value/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._5/attention/output/dense/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._5/attention/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._5/attention/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._5/attention/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._5/intermediate/dense/kernel:0 (768, 3072)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._5/intermediate/dense/bias:0 (3072,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._5/output/dense/kernel:0 (3072, 768)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._5/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._5/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._5/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._6/attention/self/query/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._6/attention/self/query/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._6/attention/self/key/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._6/attention/self/key/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._6/attention/self/value/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._6/attention/self/value/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._6/attention/output/dense/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._6/attention/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._6/attention/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._6/attention/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._6/intermediate/dense/kernel:0 (768, 3072)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._6/intermediate/dense/bias:0 (3072,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._6/output/dense/kernel:0 (3072, 768)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._6/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._6/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._6/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._7/attention/self/query/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._7/attention/self/query/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._7/attention/self/key/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._7/attention/self/key/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._7/attention/self/value/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._7/attention/self/value/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._7/attention/output/dense/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._7/attention/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._7/attention/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._7/attention/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._7/intermediate/dense/kernel:0 (768, 3072)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._7/intermediate/dense/bias:0 (3072,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._7/output/dense/kernel:0 (3072, 768)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._7/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._7/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._7/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._8/attention/self/query/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._8/attention/self/query/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._8/attention/self/key/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._8/attention/self/key/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._8/attention/self/value/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._8/attention/self/value/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._8/attention/output/dense/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._8/attention/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._8/attention/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._8/attention/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._8/intermediate/dense/kernel:0 (768, 3072)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._8/intermediate/dense/bias:0 (3072,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._8/output/dense/kernel:0 (3072, 768)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._8/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._8/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._8/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._9/attention/self/query/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._9/attention/self/query/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._9/attention/self/key/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._9/attention/self/key/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._9/attention/self/value/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._9/attention/self/value/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._9/attention/output/dense/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._9/attention/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._9/attention/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._9/attention/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._9/intermediate/dense/kernel:0 (768, 3072)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._9/intermediate/dense/bias:0 (3072,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._9/output/dense/kernel:0 (3072, 768)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._9/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._9/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._9/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._10/attention/self/query/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._10/attention/self/query/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._10/attention/self/key/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._10/attention/self/key/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._10/attention/self/value/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._10/attention/self/value/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._10/attention/output/dense/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._10/attention/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._10/attention/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._10/attention/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._10/intermediate/dense/kernel:0 (768, 3072)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._10/intermediate/dense/bias:0 (3072,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._10/output/dense/kernel:0 (3072, 768)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._10/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._10/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._10/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._11/attention/self/query/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._11/attention/self/query/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._11/attention/self/key/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._11/attention/self/key/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._11/attention/self/value/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._11/attention/self/value/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._11/attention/output/dense/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._11/attention/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._11/attention/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._11/attention/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._11/intermediate/dense/kernel:0 (768, 3072)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._11/intermediate/dense/bias:0 (3072,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._11/output/dense/kernel:0 (3072, 768)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._11/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._11/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/encoder/layer_._11/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/embeddings/word_embeddings/weight:0 (30522, 768)\n",
      "tf_roberta_for_sequence_classification_22/roberta/embeddings/token_type_embeddings/embeddings:0 (2, 768)\n",
      "tf_roberta_for_sequence_classification_22/roberta/embeddings/position_embeddings/embeddings:0 (512, 768)\n",
      "tf_roberta_for_sequence_classification_22/roberta/embeddings/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/roberta/embeddings/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/classifier/dense/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_22/classifier/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_22/classifier/out_proj/kernel:0 (768, 2)\n",
      "tf_roberta_for_sequence_classification_22/classifier/out_proj/bias:0 (2,)\n"
     ]
    }
   ],
   "source": [
    "from llm_weaver import get_model\n",
    "\n",
    "model = get_model(\"textattack/distilbert-base-uncased-RTE\")\n",
    "\n",
    "for item in model.weights:\n",
    "    print(item.name, item.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._0/attention/self/query/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._0/attention/self/query/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._0/attention/self/key/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._0/attention/self/key/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._0/attention/self/value/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._0/attention/self/value/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._0/attention/output/dense/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._0/attention/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._0/attention/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._0/attention/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._0/intermediate/dense/kernel:0 (768, 3072)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._0/intermediate/dense/bias:0 (3072,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._0/output/dense/kernel:0 (3072, 768)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._0/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._0/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._0/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._1/attention/self/query/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._1/attention/self/query/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._1/attention/self/key/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._1/attention/self/key/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._1/attention/self/value/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._1/attention/self/value/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._1/attention/output/dense/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._1/attention/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._1/attention/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._1/attention/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._1/intermediate/dense/kernel:0 (768, 3072)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._1/intermediate/dense/bias:0 (3072,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._1/output/dense/kernel:0 (3072, 768)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._1/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._1/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._1/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._2/attention/self/query/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._2/attention/self/query/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._2/attention/self/key/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._2/attention/self/key/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._2/attention/self/value/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._2/attention/self/value/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._2/attention/output/dense/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._2/attention/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._2/attention/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._2/attention/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._2/intermediate/dense/kernel:0 (768, 3072)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._2/intermediate/dense/bias:0 (3072,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._2/output/dense/kernel:0 (3072, 768)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._2/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._2/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._2/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._3/attention/self/query/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._3/attention/self/query/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._3/attention/self/key/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._3/attention/self/key/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._3/attention/self/value/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._3/attention/self/value/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._3/attention/output/dense/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._3/attention/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._3/attention/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._3/attention/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._3/intermediate/dense/kernel:0 (768, 3072)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._3/intermediate/dense/bias:0 (3072,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._3/output/dense/kernel:0 (3072, 768)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._3/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._3/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._3/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._4/attention/self/query/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._4/attention/self/query/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._4/attention/self/key/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._4/attention/self/key/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._4/attention/self/value/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._4/attention/self/value/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._4/attention/output/dense/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._4/attention/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._4/attention/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._4/attention/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._4/intermediate/dense/kernel:0 (768, 3072)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._4/intermediate/dense/bias:0 (3072,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._4/output/dense/kernel:0 (3072, 768)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._4/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._4/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._4/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._5/attention/self/query/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._5/attention/self/query/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._5/attention/self/key/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._5/attention/self/key/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._5/attention/self/value/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._5/attention/self/value/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._5/attention/output/dense/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._5/attention/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._5/attention/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._5/attention/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._5/intermediate/dense/kernel:0 (768, 3072)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._5/intermediate/dense/bias:0 (3072,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._5/output/dense/kernel:0 (3072, 768)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._5/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._5/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._5/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._6/attention/self/query/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._6/attention/self/query/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._6/attention/self/key/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._6/attention/self/key/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._6/attention/self/value/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._6/attention/self/value/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._6/attention/output/dense/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._6/attention/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._6/attention/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._6/attention/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._6/intermediate/dense/kernel:0 (768, 3072)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._6/intermediate/dense/bias:0 (3072,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._6/output/dense/kernel:0 (3072, 768)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._6/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._6/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._6/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._7/attention/self/query/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._7/attention/self/query/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._7/attention/self/key/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._7/attention/self/key/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._7/attention/self/value/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._7/attention/self/value/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._7/attention/output/dense/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._7/attention/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._7/attention/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._7/attention/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._7/intermediate/dense/kernel:0 (768, 3072)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._7/intermediate/dense/bias:0 (3072,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._7/output/dense/kernel:0 (3072, 768)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._7/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._7/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._7/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._8/attention/self/query/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._8/attention/self/query/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._8/attention/self/key/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._8/attention/self/key/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._8/attention/self/value/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._8/attention/self/value/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._8/attention/output/dense/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._8/attention/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._8/attention/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._8/attention/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._8/intermediate/dense/kernel:0 (768, 3072)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._8/intermediate/dense/bias:0 (3072,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._8/output/dense/kernel:0 (3072, 768)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._8/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._8/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._8/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._9/attention/self/query/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._9/attention/self/query/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._9/attention/self/key/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._9/attention/self/key/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._9/attention/self/value/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._9/attention/self/value/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._9/attention/output/dense/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._9/attention/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._9/attention/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._9/attention/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._9/intermediate/dense/kernel:0 (768, 3072)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._9/intermediate/dense/bias:0 (3072,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._9/output/dense/kernel:0 (3072, 768)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._9/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._9/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._9/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._10/attention/self/query/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._10/attention/self/query/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._10/attention/self/key/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._10/attention/self/key/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._10/attention/self/value/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._10/attention/self/value/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._10/attention/output/dense/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._10/attention/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._10/attention/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._10/attention/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._10/intermediate/dense/kernel:0 (768, 3072)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._10/intermediate/dense/bias:0 (3072,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._10/output/dense/kernel:0 (3072, 768)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._10/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._10/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._10/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._11/attention/self/query/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._11/attention/self/query/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._11/attention/self/key/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._11/attention/self/key/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._11/attention/self/value/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._11/attention/self/value/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._11/attention/output/dense/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._11/attention/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._11/attention/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._11/attention/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._11/intermediate/dense/kernel:0 (768, 3072)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._11/intermediate/dense/bias:0 (3072,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._11/output/dense/kernel:0 (3072, 768)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._11/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._11/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/encoder/layer_._11/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/embeddings/word_embeddings/weight:0 (50265, 768)\n",
      "tf_roberta_for_sequence_classification_26/roberta/embeddings/token_type_embeddings/embeddings:0 (1, 768)\n",
      "tf_roberta_for_sequence_classification_26/roberta/embeddings/position_embeddings/embeddings:0 (514, 768)\n",
      "tf_roberta_for_sequence_classification_26/roberta/embeddings/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/roberta/embeddings/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/classifier/dense/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_26/classifier/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_26/classifier/out_proj/kernel:0 (768, 2)\n",
      "tf_roberta_for_sequence_classification_26/classifier/out_proj/bias:0 (2,)\n"
     ]
    }
   ],
   "source": [
    "from llm_weaver import get_model\n",
    "\n",
    "model = get_model(\"textattack/roberta-base-RTE\")\n",
    "\n",
    "for item in model.weights:\n",
    "    print(item.name, item.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_weaver import get_model\n",
    "\n",
    "model = get_model(\"textattack/roberta-base-RTE\")\n",
    "\n",
    "for item in model.weights:\n",
    "    print(item.name, item.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"get-multi-task-scores.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task</th>\n",
       "      <th>classification_head_model</th>\n",
       "      <th>layers_models</th>\n",
       "      <th>split</th>\n",
       "      <th>n_examples</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>mnli</td>\n",
       "      <td>JeremiahZ/roberta-base-mnli</td>\n",
       "      <td>[JeremiahZ/roberta-base-mnli]</td>\n",
       "      <td>validation</td>\n",
       "      <td>256</td>\n",
       "      <td>0.855469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>rte</td>\n",
       "      <td>JeremiahZ/roberta-base-rte</td>\n",
       "      <td>[JeremiahZ/roberta-base-mnli]</td>\n",
       "      <td>validation</td>\n",
       "      <td>256</td>\n",
       "      <td>0.746094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rte</td>\n",
       "      <td>textattack/roberta-base-RTE</td>\n",
       "      <td>[textattack/roberta-base-RTE]</td>\n",
       "      <td>validation</td>\n",
       "      <td>256</td>\n",
       "      <td>0.726562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>mnli</td>\n",
       "      <td>howey/roberta-large-mnli</td>\n",
       "      <td>[howey/roberta-large-mnli]</td>\n",
       "      <td>validation</td>\n",
       "      <td>256</td>\n",
       "      <td>0.707031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rte</td>\n",
       "      <td>howey/roberta-large-rte</td>\n",
       "      <td>[howey/roberta-large-rte]</td>\n",
       "      <td>validation</td>\n",
       "      <td>256</td>\n",
       "      <td>0.644531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>rte</td>\n",
       "      <td>JeremiahZ/roberta-base-rte</td>\n",
       "      <td>[JeremiahZ/roberta-base-rte]</td>\n",
       "      <td>validation</td>\n",
       "      <td>256</td>\n",
       "      <td>0.621094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rte</td>\n",
       "      <td>textattack/roberta-base-RTE</td>\n",
       "      <td>[textattack/roberta-base-MNLI]</td>\n",
       "      <td>validation</td>\n",
       "      <td>256</td>\n",
       "      <td>0.515625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>mnli</td>\n",
       "      <td>howey/roberta-large-mnli</td>\n",
       "      <td>[howey/roberta-large-rte]</td>\n",
       "      <td>validation</td>\n",
       "      <td>256</td>\n",
       "      <td>0.390625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>mnli</td>\n",
       "      <td>JeremiahZ/roberta-base-mnli</td>\n",
       "      <td>[JeremiahZ/roberta-base-rte]</td>\n",
       "      <td>validation</td>\n",
       "      <td>256</td>\n",
       "      <td>0.289062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mnli</td>\n",
       "      <td>textattack/roberta-base-MNLI</td>\n",
       "      <td>[textattack/roberta-base-MNLI]</td>\n",
       "      <td>validation</td>\n",
       "      <td>256</td>\n",
       "      <td>0.265625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mnli</td>\n",
       "      <td>textattack/roberta-base-MNLI</td>\n",
       "      <td>[textattack/roberta-base-RTE]</td>\n",
       "      <td>validation</td>\n",
       "      <td>256</td>\n",
       "      <td>0.261719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>rte</td>\n",
       "      <td>howey/roberta-large-rte</td>\n",
       "      <td>[howey/roberta-large-mnli]</td>\n",
       "      <td>validation</td>\n",
       "      <td>256</td>\n",
       "      <td>0.238281</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    task     classification_head_model                   layers_models  \\\n",
       "11  mnli   JeremiahZ/roberta-base-mnli   [JeremiahZ/roberta-base-mnli]   \n",
       "9    rte    JeremiahZ/roberta-base-rte   [JeremiahZ/roberta-base-mnli]   \n",
       "0    rte   textattack/roberta-base-RTE   [textattack/roberta-base-RTE]   \n",
       "7   mnli      howey/roberta-large-mnli      [howey/roberta-large-mnli]   \n",
       "4    rte       howey/roberta-large-rte       [howey/roberta-large-rte]   \n",
       "8    rte    JeremiahZ/roberta-base-rte    [JeremiahZ/roberta-base-rte]   \n",
       "1    rte   textattack/roberta-base-RTE  [textattack/roberta-base-MNLI]   \n",
       "6   mnli      howey/roberta-large-mnli       [howey/roberta-large-rte]   \n",
       "10  mnli   JeremiahZ/roberta-base-mnli    [JeremiahZ/roberta-base-rte]   \n",
       "3   mnli  textattack/roberta-base-MNLI  [textattack/roberta-base-MNLI]   \n",
       "2   mnli  textattack/roberta-base-MNLI   [textattack/roberta-base-RTE]   \n",
       "5    rte       howey/roberta-large-rte      [howey/roberta-large-mnli]   \n",
       "\n",
       "         split  n_examples  accuracy  \n",
       "11  validation         256  0.855469  \n",
       "9   validation         256  0.746094  \n",
       "0   validation         256  0.726562  \n",
       "7   validation         256  0.707031  \n",
       "4   validation         256  0.644531  \n",
       "8   validation         256  0.621094  \n",
       "1   validation         256  0.515625  \n",
       "6   validation         256  0.390625  \n",
       "10  validation         256  0.289062  \n",
       "3   validation         256  0.265625  \n",
       "2   validation         256  0.261719  \n",
       "5   validation         256  0.238281  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values([\"accuracy\", \"task\", \"split\"], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
