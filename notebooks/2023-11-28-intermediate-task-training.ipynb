{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-end notebook!\n",
    "\n",
    "Here is the workflow:\n",
    "* Sampling configs (sampling parameters, etc.) lead to...\n",
    "* Weaving configs (blank model settings, donor model settings, layer assignments) lead to...\n",
    "* Models (probably TFRobertaForSequenceClassification in all cases) lead to...\n",
    "* Performance scores (numbers from 0-100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: visions 0.7.5 does not provide the extra 'type-image-path'\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# install dependencies\n",
    "\n",
    "! pip install -q joblib  # joblib for memoizing functions\n",
    "! pip install -q ipywidgets widgetsnbextension pandas-profiling # IProgress for progress bars\n",
    "\n",
    "# ! pip install -q tensorflow==2.13.0 tensorflow-datasets==4.9.2 tensorflow-probability==0.21.0 transformers==4.35.0  datasets==2.14.6 torch==2.1.0 scipy==1.10.1 scikit-learn==1.3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add model_merging to the python path\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "model_merging_base = os.path.abspath(\"../model_merging/\")\n",
    "# assert it exist\n",
    "assert os.path.exists(model_merging_base)\n",
    "if model_merging_base not in sys.path:\n",
    "    sys.path.append(model_merging_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import joblib for caching and distributed computing\n",
    "from math import sqrt\n",
    "\n",
    "from joblib import Memory, Parallel, delayed\n",
    "\n",
    "memory = Memory(location=\"cache\", verbose=10)\n",
    "\n",
    "parallel = Parallel(n_jobs=2, return_as=\"generator\")\n",
    "output_generator = parallel(delayed(sqrt)(i**2) for i in range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and cached functions\n",
    "\n",
    "import os\n",
    "\n",
    "from llm_weaver import (\n",
    "    calculate_score_from_weaving_config,\n",
    "    test_weaver,\n",
    ")\n",
    "\n",
    "# Disable parallelism in tokenizers to avoid deadlocks\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "calculate_score_from_weaving_config_cached = memory.cache(\n",
    "    calculate_score_from_weaving_config\n",
    ")\n",
    "test_weaver_cached = memory.cache(test_weaver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Get RTE scores\n",
    "\n",
    "* RTE vanilla\n",
    "* RTE isotropically merged with MNLI score with a weight chosen properly\n",
    "* RTE fisher merge with MNLI with a weight chosen properly\n",
    "* replacing with certain layers?\n",
    "* Shifting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps: configs to graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"textAttack/roberta-base-RTE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/5dd88dc88dcbcca6c00d08955e614e0d\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RTEVanilla</td>\n",
       "      <td>0.726562</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         name  accuracy\n",
       "0  RTEVanilla  0.726562"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from llm_weaver import dict_overwrite, get_model_config, normalize_glue_task_name\n",
    "\n",
    "model_id = \"textAttack/roberta-base-RTE\"\n",
    "\n",
    "\n",
    "def RTEVanilla(model_id):\n",
    "    num_layers = get_model_config(model_id)[\"num_hidden_layers\"]\n",
    "    layer_assignments = [\n",
    "        {\n",
    "            \"type\": \"SingleLayer\",\n",
    "            \"params\": {\n",
    "                \"donor\": model_id,\n",
    "                \"hidden_layer_number\": i,\n",
    "            },\n",
    "        }\n",
    "        for i in range(num_layers)\n",
    "    ]\n",
    "\n",
    "    blank_model_config = dict_overwrite(\n",
    "        get_model_config(model_id),\n",
    "        {\n",
    "            \"num_hidden_layers\": len(layer_assignments),\n",
    "        },\n",
    "    )\n",
    "    config = {\n",
    "        \"glue_task\": normalize_glue_task_name(model_id),\n",
    "        \"tokenizer_model_id\": model_id,\n",
    "        \"blank_model_config\": blank_model_config,\n",
    "        \"layer_assignments\": layer_assignments,\n",
    "        \"classification_head\": {\n",
    "            \"type\": \"SingleClassificationHead\",\n",
    "            \"params\": {\n",
    "                \"donor\": model_id,\n",
    "            },\n",
    "        },\n",
    "        \"embeddings\": {\n",
    "            \"type\": \"SingleEmbeddings\",\n",
    "            \"params\": {\n",
    "                \"donor\": model_id,\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "\n",
    "    yield config\n",
    "\n",
    "\n",
    "weave_configs = list(RTEVanilla(model_id))\n",
    "\n",
    "scores = Parallel(n_jobs=5, return_as=\"list\")(\n",
    "    delayed(calculate_score_from_weaving_config_cached)(\n",
    "        weave_config,\n",
    "        # n_examples=4096,\n",
    "        n_examples=128,\n",
    "        split=\"validation\",\n",
    "    )\n",
    "    for weave_config in weave_configs\n",
    ")\n",
    "accuracies = [score[\"accuracy\"] for score in scores]\n",
    "\n",
    "records = []\n",
    "for weave_config, accuracy in zip(weave_configs, accuracies):\n",
    "    record = {}\n",
    "    record[\"name\"] = \"RTEVanilla\"\n",
    "    record[\"accuracy\"] = accuracy\n",
    "    records.append(record)\n",
    "df_rte_vanilla = pd.DataFrame.from_records(records)\n",
    "df_rte_vanilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=128, split='validation')\n",
      "calculating score for weaving config md5sum: 4211acb62720b8f6ecd6452e8fe39dfc\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=128, split='validation')\n",
      "calculating score for weaving config md5sum: 3aa46fab0c1eff67801c7f57142f6878\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=128, split='validation')\n",
      "calculating score for weaving config md5sum: e339726b1e909bd0a678afaf8a785478\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=128, split='validation')\n",
      "calculating score for weaving config md5sum: 069c53d14142f897dda0929a5e226251\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=128, split='validation')\n",
      "calculating score for weaving config md5sum: ae89970b475233a6acba476526d27a4e\n",
      "Loading textAttack/roberta-base-MNLI\n",
      "Loading textAttack/roberta-base-RTE\n",
      "Loading textAttack/roberta-base-MNLI\n",
      "Loading textAttack/roberta-base-MNLI\n",
      "Loading textAttack/roberta-base-RTE\n",
      "Loading textAttack/roberta-base-MNLI\n",
      "Loading textAttack/roberta-base-MNLI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading textAttack/roberta-base-RTE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading textAttack/roberta-base-RTE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading textAttack/roberta-base-RTE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/briancruz/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "/Users/briancruz/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "/Users/briancruz/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "/Users/briancruz/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "/Users/briancruz/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "2023-11-28 14:35:09.015958: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-11-28 14:35:11.035174: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-11-28 14:35:12.356048: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-11-28 14:35:13.334702: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-11-28 14:35:15.573333: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________________________calculate_score_from_weaving_config - 63.1s, 1.1min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=128, split='validation')\n",
      "calculating score for weaving config md5sum: 25b4edc2414e66517e663096f5e6bbdf\n",
      "_____________________________calculate_score_from_weaving_config - 65.7s, 1.1min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=128, split='validation')\n",
      "calculating score for weaving config md5sum: d6064f2db2cf67d54540a14f501c4363\n",
      "_____________________________calculate_score_from_weaving_config - 66.6s, 1.1min\n",
      "_____________________________calculate_score_from_weaving_config - 67.5s, 1.1min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=128, split='validation')\n",
      "calculating score for weaving config md5sum: e21e3b35d8de4a2640e7f993813c59e4\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=128, split='validation')\n",
      "calculating score for weaving config md5sum: a5a3ffc722409155abf6a1e8e4bdd991\n",
      "Loading textAttack/roberta-base-MNLI\n",
      "Loading textAttack/roberta-base-RTE\n",
      "_____________________________calculate_score_from_weaving_config - 69.4s, 1.2min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=128, split='validation')\n",
      "calculating score for weaving config md5sum: 612f1ff6aa3c93dafd7ae8502625e4c5\n",
      "Loading textAttack/roberta-base-RTE\n",
      "Loading textAttack/roberta-base-MNLI\n",
      "Loading textAttack/roberta-base-MNLI\n",
      "Loading textAttack/roberta-base-RTE\n",
      "Loading textAttack/roberta-base-MNLI\n",
      "Loading textAttack/roberta-base-RTE\n",
      "Loading textAttack/roberta-base-RTE\n",
      "Loading textAttack/roberta-base-MNLI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/briancruz/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "/Users/briancruz/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "/Users/briancruz/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "/Users/briancruz/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "/Users/briancruz/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "2023-11-28 14:36:04.328508: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-11-28 14:36:08.480609: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-11-28 14:36:15.703373: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-11-28 14:36:18.311929: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-11-28 14:36:21.196369: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________________________calculate_score_from_weaving_config - 53.8s, 0.9min\n",
      "_____________________________calculate_score_from_weaving_config - 56.6s, 0.9min\n",
      "_____________________________calculate_score_from_weaving_config - 61.1s, 1.0min\n",
      "_____________________________calculate_score_from_weaving_config - 64.3s, 1.1min\n",
      "_____________________________calculate_score_from_weaving_config - 65.4s, 1.1min\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RTEMNLIIsotropic</td>\n",
       "      <td>0.531250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RTEMNLIIsotropic</td>\n",
       "      <td>0.562500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RTEMNLIIsotropic</td>\n",
       "      <td>0.562500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RTEMNLIIsotropic</td>\n",
       "      <td>0.578125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RTEMNLIIsotropic</td>\n",
       "      <td>0.585938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RTEMNLIIsotropic</td>\n",
       "      <td>0.609375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RTEMNLIIsotropic</td>\n",
       "      <td>0.742188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RTEMNLIIsotropic</td>\n",
       "      <td>0.734375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RTEMNLIIsotropic</td>\n",
       "      <td>0.726562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>RTEMNLIIsotropic</td>\n",
       "      <td>0.726562</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               name  accuracy\n",
       "0  RTEMNLIIsotropic  0.531250\n",
       "1  RTEMNLIIsotropic  0.562500\n",
       "2  RTEMNLIIsotropic  0.562500\n",
       "3  RTEMNLIIsotropic  0.578125\n",
       "4  RTEMNLIIsotropic  0.585938\n",
       "5  RTEMNLIIsotropic  0.609375\n",
       "6  RTEMNLIIsotropic  0.742188\n",
       "7  RTEMNLIIsotropic  0.734375\n",
       "8  RTEMNLIIsotropic  0.726562\n",
       "9  RTEMNLIIsotropic  0.726562"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from llm_weaver import dict_overwrite, get_model_config, normalize_glue_task_name\n",
    "\n",
    "model_id = \"textAttack/roberta-base-RTE\"\n",
    "\n",
    "\n",
    "def RTEMNLIIsotropic(model_id):\n",
    "    for alpha in [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.7, 0.8, 0.9, 1.0]:\n",
    "        num_layers = get_model_config(model_id)[\"num_hidden_layers\"]\n",
    "        layer_assignments = [\n",
    "            {\n",
    "                \"type\": \"IsotropicLinearCombination\",\n",
    "                \"params\": {\n",
    "                    \"donors\": [\n",
    "                        {\"donor\": model_id, \"hidden_layer_number\": i, \"weight\": alpha},\n",
    "                        {\n",
    "                            \"donor\": \"textAttack/roberta-base-MNLI\",\n",
    "                            \"hidden_layer_number\": i,\n",
    "                            \"weight\": 1.0 - alpha,\n",
    "                        },\n",
    "                    ]\n",
    "                },\n",
    "            }\n",
    "            for i in range(num_layers)\n",
    "        ]\n",
    "\n",
    "        blank_model_config = dict_overwrite(\n",
    "            get_model_config(model_id),\n",
    "            {\n",
    "                \"num_hidden_layers\": len(layer_assignments),\n",
    "            },\n",
    "        )\n",
    "        config = {\n",
    "            \"glue_task\": normalize_glue_task_name(model_id),\n",
    "            \"tokenizer_model_id\": model_id,\n",
    "            \"blank_model_config\": blank_model_config,\n",
    "            \"layer_assignments\": layer_assignments,\n",
    "            \"classification_head\": {\n",
    "                \"type\": \"SingleClassificationHead\",\n",
    "                \"params\": {\n",
    "                    \"donor\": model_id,\n",
    "                },\n",
    "            },\n",
    "            \"embeddings\": {\n",
    "                \"type\": \"SingleEmbeddings\",\n",
    "                \"params\": {\n",
    "                    \"donor\": model_id,\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "\n",
    "        yield config\n",
    "\n",
    "\n",
    "weave_configs = list(RTEMNLIIsotropic(model_id))\n",
    "\n",
    "scores = Parallel(n_jobs=5, return_as=\"list\")(\n",
    "    delayed(calculate_score_from_weaving_config_cached)(\n",
    "        weave_config,\n",
    "        # n_examples=4096,\n",
    "        n_examples=128,\n",
    "        split=\"validation\",\n",
    "    )\n",
    "    for weave_config in weave_configs\n",
    ")\n",
    "accuracies = [score[\"accuracy\"] for score in scores]\n",
    "\n",
    "records = []\n",
    "for weave_config, accuracy in zip(weave_configs, accuracies):\n",
    "    record = {}\n",
    "    record[\"name\"] = \"RTEMNLIIsotropic\"\n",
    "    record[\"accuracy\"] = accuracy\n",
    "    records.append(record)\n",
    "df_rte_vanilla = pd.DataFrame.from_records(records)\n",
    "df_rte_vanilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=128, split='validation')\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=128, split='validation')\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=128, split='validation')\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=128, split='validation')\n",
      "calculating score for weaving config md5sum: 648c954745a14670f1e52e6eedd841d3\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=128, split='validation')\n",
      "calculating score for weaving config md5sum: 0151c46f2cf6223257511b55a485a01b\n",
      "calculating score for weaving config md5sum: 4f16814bbfa8f4c3e6c41e225128f05c\n",
      "calculating score for weaving config md5sum: a520971f470ac9c68e8497a06f86fb02\n",
      "calculating score for weaving config md5sum: c74be055e751e3191bdf31cafaa566fb\n",
      "Loading textAttack/roberta-base-RTE\n",
      "Loading textAttack/roberta-base-MNLI\n",
      "Loading textAttack/roberta-base-RTE\n",
      "Loading textAttack/roberta-base-MNLI\n",
      "Loading textAttack/roberta-base-MNLI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading textAttack/roberta-base-MNLI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading textAttack/roberta-base-RTE\n",
      "Loading textAttack/roberta-base-MNLI\n",
      "Loading textAttack/roberta-base-RTE\n",
      "Loading textAttack/roberta-base-RTE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "/Users/briancruz/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "/Users/briancruz/2023-fall-cs-194-294-merging-llms/model_merging/model_merging/evaluation.py:7: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  return hfds.load_metric(\"glue\", task)\n",
      "/Users/briancruz/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "/Users/briancruz/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "/Users/briancruz/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "/Users/briancruz/2023-fall-cs-194-294-merging-llms/model_merging/model_merging/evaluation.py:7: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  return hfds.load_metric(\"glue\", task)\n",
      "/Users/briancruz/2023-fall-cs-194-294-merging-llms/model_merging/model_merging/evaluation.py:7: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  return hfds.load_metric(\"glue\", task)\n",
      "/Users/briancruz/2023-fall-cs-194-294-merging-llms/model_merging/model_merging/evaluation.py:7: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  return hfds.load_metric(\"glue\", task)\n",
      "/Users/briancruz/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "/Users/briancruz/2023-fall-cs-194-294-merging-llms/model_merging/model_merging/evaluation.py:7: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  return hfds.load_metric(\"glue\", task)\n",
      "2023-11-28 15:07:48.201946: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-11-28 15:07:49.266199: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-11-28 15:07:49.831515: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-11-28 15:07:49.883992: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-11-28 15:07:49.950869: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________________________calculate_score_from_weaving_config - 29.5s, 0.5min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=128, split='validation')\n",
      "calculating score for weaving config md5sum: 8ff67a04fdc19df6d64a3acaae7eb457\n",
      "_____________________________calculate_score_from_weaving_config - 31.8s, 0.5min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=128, split='validation')\n",
      "calculating score for weaving config md5sum: b058223e0207bef1205b7994e38a1ce8\n",
      "Loading textAttack/roberta-base-MNLI\n",
      "Loading textAttack/roberta-base-RTE\n",
      "_____________________________calculate_score_from_weaving_config - 32.9s, 0.5min\n",
      "_____________________________calculate_score_from_weaving_config - 33.0s, 0.5min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=128, split='validation')\n",
      "calculating score for weaving config md5sum: 9fba571f27543d2705cb0ca3de5516c7\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=128, split='validation')\n",
      "calculating score for weaving config md5sum: 2bddf36358b01d87e1fff56974376fe2\n",
      "_____________________________calculate_score_from_weaving_config - 33.3s, 0.6min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/8051cc6518ef81517d34661c7f434bf9\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "Loading textAttack/roberta-base-RTE\n",
      "Loading textAttack/roberta-base-MNLI\n",
      "Loading textAttack/roberta-base-RTE\n",
      "Loading textAttack/roberta-base-MNLI\n",
      "Loading textAttack/roberta-base-MNLI\n",
      "Loading textAttack/roberta-base-RTE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/briancruz/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "/Users/briancruz/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "/Users/briancruz/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "/Users/briancruz/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "2023-11-28 15:08:11.733240: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-11-28 15:08:14.638119: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-11-28 15:08:15.252743: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-11-28 15:08:15.320556: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________________________calculate_score_from_weaving_config - 21.9s, 0.4min\n",
      "_____________________________calculate_score_from_weaving_config - 22.9s, 0.4min\n",
      "_____________________________calculate_score_from_weaving_config - 22.4s, 0.4min\n",
      "_____________________________calculate_score_from_weaving_config - 22.6s, 0.4min\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RTEMNLIIsotropicMarenLayers</td>\n",
       "      <td>0.710938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RTEMNLIIsotropicMarenLayers</td>\n",
       "      <td>0.734375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RTEMNLIIsotropicMarenLayers</td>\n",
       "      <td>0.734375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RTEMNLIIsotropicMarenLayers</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RTEMNLIIsotropicMarenLayers</td>\n",
       "      <td>0.773438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RTEMNLIIsotropicMarenLayers</td>\n",
       "      <td>0.765625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RTEMNLIIsotropicMarenLayers</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RTEMNLIIsotropicMarenLayers</td>\n",
       "      <td>0.734375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RTEMNLIIsotropicMarenLayers</td>\n",
       "      <td>0.734375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>RTEMNLIIsotropicMarenLayers</td>\n",
       "      <td>0.726562</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          name  accuracy\n",
       "0  RTEMNLIIsotropicMarenLayers  0.710938\n",
       "1  RTEMNLIIsotropicMarenLayers  0.734375\n",
       "2  RTEMNLIIsotropicMarenLayers  0.734375\n",
       "3  RTEMNLIIsotropicMarenLayers  0.750000\n",
       "4  RTEMNLIIsotropicMarenLayers  0.773438\n",
       "5  RTEMNLIIsotropicMarenLayers  0.765625\n",
       "6  RTEMNLIIsotropicMarenLayers  0.750000\n",
       "7  RTEMNLIIsotropicMarenLayers  0.734375\n",
       "8  RTEMNLIIsotropicMarenLayers  0.734375\n",
       "9  RTEMNLIIsotropicMarenLayers  0.726562"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from llm_weaver import dict_overwrite, get_model_config, normalize_glue_task_name\n",
    "\n",
    "model_id = \"textAttack/roberta-base-RTE\"\n",
    "\n",
    "\n",
    "def RTEMNLIIsotropicMarenLayers(model_id):\n",
    "    replacement_layers = [0, 1, 4, 11]\n",
    "    for alpha in [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.7, 0.8, 0.9, 1.0]:\n",
    "        num_layers = get_model_config(model_id)[\"num_hidden_layers\"]\n",
    "        layer_assignments = [\n",
    "            {\n",
    "                \"type\": \"IsotropicLinearCombination\",\n",
    "                \"params\": {\n",
    "                    \"donors\": [\n",
    "                        {\n",
    "                            \"donor\": model_id,\n",
    "                            \"hidden_layer_number\": i,\n",
    "                            \"weight\": alpha if (i in replacement_layers) else 1.0,\n",
    "                        },\n",
    "                        {\n",
    "                            \"donor\": \"textAttack/roberta-base-MNLI\",\n",
    "                            \"hidden_layer_number\": i,\n",
    "                            \"weight\": (1.0 - alpha)\n",
    "                            if (i in replacement_layers)\n",
    "                            else 0.0,\n",
    "                        },\n",
    "                    ]\n",
    "                },\n",
    "            }\n",
    "            for i in range(num_layers)\n",
    "        ]\n",
    "\n",
    "        blank_model_config = dict_overwrite(\n",
    "            get_model_config(model_id),\n",
    "            {\n",
    "                \"num_hidden_layers\": len(layer_assignments),\n",
    "            },\n",
    "        )\n",
    "        config = {\n",
    "            \"glue_task\": normalize_glue_task_name(model_id),\n",
    "            \"tokenizer_model_id\": model_id,\n",
    "            \"blank_model_config\": blank_model_config,\n",
    "            \"layer_assignments\": layer_assignments,\n",
    "            \"classification_head\": {\n",
    "                \"type\": \"SingleClassificationHead\",\n",
    "                \"params\": {\n",
    "                    \"donor\": model_id,\n",
    "                },\n",
    "            },\n",
    "            \"embeddings\": {\n",
    "                \"type\": \"SingleEmbeddings\",\n",
    "                \"params\": {\n",
    "                    \"donor\": model_id,\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "\n",
    "        yield config\n",
    "\n",
    "\n",
    "weave_configs = list(RTEMNLIIsotropicMarenLayers(model_id))\n",
    "\n",
    "scores = Parallel(n_jobs=5, return_as=\"list\")(\n",
    "    delayed(calculate_score_from_weaving_config_cached)(\n",
    "        weave_config,\n",
    "        # n_examples=4096,\n",
    "        n_examples=128,\n",
    "        split=\"validation\",\n",
    "    )\n",
    "    for weave_config in weave_configs\n",
    ")\n",
    "accuracies = [score[\"accuracy\"] for score in scores]\n",
    "\n",
    "records = []\n",
    "for weave_config, accuracy in zip(weave_configs, accuracies):\n",
    "    record = {}\n",
    "    record[\"name\"] = \"RTEMNLIIsotropicMarenLayers\"\n",
    "    record[\"accuracy\"] = accuracy\n",
    "    records.append(record)\n",
    "df_rte_vanilla = pd.DataFrame.from_records(records)\n",
    "df_rte_vanilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=129, split='validation')\n",
      "calculating score for weaving config md5sum: b6705706a1a9c88694c11ef5abe3e3d6\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=129, split='validation')\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=129, split='validation')\n",
      "calculating score for weaving config md5sum: 52a1f123eace7e66672855094f18eca3\n",
      "calculating score for weaving config md5sum: 97842b3c7f6641b2dd544a343e92698c\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=129, split='validation')\n",
      "calculating score for weaving config md5sum: 0b1780e4332fedf9fe05edf83a221b95\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=129, split='validation')\n",
      "calculating score for weaving config md5sum: 9a8675b072f79e617d3586673149fd53\n",
      "Loading textAttack/roberta-base-MNLI\n",
      "Loading textAttack/roberta-base-RTE\n",
      "Loading textAttack/roberta-base-MNLI\n",
      "Loading textAttack/roberta-base-RTE\n",
      "Loading textAttack/roberta-base-RTE\n",
      "Loading textAttack/roberta-base-MNLI\n",
      "Loading textAttack/roberta-base-RTE\n",
      "Loading textAttack/roberta-base-MNLI\n",
      "Loading textAttack/roberta-base-RTE\n",
      "Loading textAttack/roberta-base-MNLI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/briancruz/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "/Users/briancruz/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "/Users/briancruz/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "/Users/briancruz/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "/Users/briancruz/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "2023-11-28 15:24:56.487792: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-11-28 15:24:57.593469: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-11-28 15:24:57.987041: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-11-28 15:24:58.117088: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-11-28 15:24:58.217604: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: All predictions are the same! Is your model broken? [0]\n",
      "_____________________________calculate_score_from_weaving_config - 27.1s, 0.5min\n",
      "WARNING: All predictions are the same! Is your model broken? [0]\n",
      "_____________________________calculate_score_from_weaving_config - 29.1s, 0.5min\n",
      "WARNING: All predictions are the same! Is your model broken? [0]\n",
      "_____________________________calculate_score_from_weaving_config - 29.2s, 0.5min\n",
      "WARNING: All predictions are the same! Is your model broken? [0]\n",
      "WARNING: All predictions are the same! Is your model broken? [0]\n",
      "_____________________________calculate_score_from_weaving_config - 29.0s, 0.5min\n",
      "_____________________________calculate_score_from_weaving_config - 29.3s, 0.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/briancruz/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=129, split='validation')________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=129, split='validation')\n",
      "\n",
      "calculating score for weaving config md5sum: 37eb6b140617cc87446c5f826cb0e385\n",
      "calculating score for weaving config md5sum: beaff76339c62772e81a0394ab989949\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=129, split='validation')\n",
      "calculating score for weaving config md5sum: b0687f7a578faf28421a1fda03ea468b\n",
      "Loading textAttack/roberta-base-MNLI\n",
      "Loading textAttack/roberta-base-MNLI\n",
      "Loading textAttack/roberta-base-MNLI\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=129, split='validation')\n",
      "calculating score for weaving config md5sum: 1ee8afe250fe937098592c76bd26309f\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=129, split='validation')\n",
      "calculating score for weaving config md5sum: e0e94f5a11deba5a8bcf3ed46ff3c5db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading textAttack/roberta-base-RTE\n",
      "Loading textAttack/roberta-base-RTE\n",
      "Loading textAttack/roberta-base-RTE\n",
      "Loading textAttack/roberta-base-RTE\n",
      "Loading textAttack/roberta-base-MNLI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading textAttack/roberta-base-MNLI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading textAttack/roberta-base-RTE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "/Users/briancruz/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "/Users/briancruz/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "/Users/briancruz/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "/Users/briancruz/2023-fall-cs-194-294-merging-llms/model_merging/model_merging/evaluation.py:7: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  return hfds.load_metric(\"glue\", task)\n",
      "/Users/briancruz/2023-fall-cs-194-294-merging-llms/model_merging/model_merging/evaluation.py:7: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  return hfds.load_metric(\"glue\", task)\n",
      "/Users/briancruz/2023-fall-cs-194-294-merging-llms/model_merging/model_merging/evaluation.py:7: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  return hfds.load_metric(\"glue\", task)\n",
      "/Users/briancruz/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "/Users/briancruz/2023-fall-cs-194-294-merging-llms/model_merging/model_merging/evaluation.py:7: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  return hfds.load_metric(\"glue\", task)\n",
      "/Users/briancruz/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "/Users/briancruz/2023-fall-cs-194-294-merging-llms/model_merging/model_merging/evaluation.py:7: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  return hfds.load_metric(\"glue\", task)\n",
      "2023-11-28 15:25:35.221608: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-11-28 15:25:36.885908: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-11-28 15:25:36.933729: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-11-28 15:25:39.113950: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-11-28 15:25:39.260589: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: All predictions are the same! Is your model broken? [0]\n",
      "_____________________________calculate_score_from_weaving_config - 32.8s, 0.5min\n",
      "WARNING: All predictions are the same! Is your model broken? [0]\n",
      "_____________________________calculate_score_from_weaving_config - 34.4s, 0.6min\n",
      "WARNING: All predictions are the same! Is your model broken? [0]\n",
      "_____________________________calculate_score_from_weaving_config - 34.0s, 0.6min\n",
      "WARNING: All predictions are the same! Is your model broken? [0]\n",
      "_____________________________calculate_score_from_weaving_config - 33.7s, 0.6min\n",
      "WARNING: All predictions are the same! Is your model broken? [0]\n",
      "_____________________________calculate_score_from_weaving_config - 32.8s, 0.5min\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FisherAllLayers</td>\n",
       "      <td>0.534884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FisherAllLayers</td>\n",
       "      <td>0.565891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FisherAllLayers</td>\n",
       "      <td>0.565891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FisherAllLayers</td>\n",
       "      <td>0.581395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FisherAllLayers</td>\n",
       "      <td>0.589147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>FisherAllLayers</td>\n",
       "      <td>0.612403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>FisherAllLayers</td>\n",
       "      <td>0.744186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>FisherAllLayers</td>\n",
       "      <td>0.744186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>FisherAllLayers</td>\n",
       "      <td>0.728682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>FisherAllLayers</td>\n",
       "      <td>0.728682</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              name  accuracy\n",
       "0  FisherAllLayers  0.534884\n",
       "1  FisherAllLayers  0.565891\n",
       "2  FisherAllLayers  0.565891\n",
       "3  FisherAllLayers  0.581395\n",
       "4  FisherAllLayers  0.589147\n",
       "5  FisherAllLayers  0.612403\n",
       "6  FisherAllLayers  0.744186\n",
       "7  FisherAllLayers  0.744186\n",
       "8  FisherAllLayers  0.728682\n",
       "9  FisherAllLayers  0.728682"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from llm_weaver import dict_overwrite, get_model_config, normalize_glue_task_name\n",
    "\n",
    "model_id = \"textAttack/roberta-base-RTE\"\n",
    "\n",
    "\n",
    "def FisherAllLayers(model_id):\n",
    "    for alpha in [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.7, 0.8, 0.9, 1.0]:\n",
    "        num_layers = get_model_config(model_id)[\"num_hidden_layers\"]\n",
    "        layer_assignments = [\n",
    "            {\n",
    "                \"type\": \"ElementWiseLinearCombination\",\n",
    "                \"params\": {\n",
    "                    \"donors\": [\n",
    "                        {\n",
    "                            \"donor\": model_id,\n",
    "                            \"hidden_layer_number\": i,\n",
    "                            \"weight\": alpha,\n",
    "                            \"element_wise_multiplier_filename\": f\"../data/fisher_info/{model_id.replace('/', '_')}-fisher-info.h5\",\n",
    "                        },\n",
    "                        {\n",
    "                            \"donor\": \"textAttack/roberta-base-MNLI\",\n",
    "                            \"hidden_layer_number\": i,\n",
    "                            \"weight\": 1.0 - alpha,\n",
    "                            \"element_wise_multiplier_filename\": \"../data/fisher_info/textAttack_roberta-base-MNLI-fisher-info.h5\",\n",
    "                        },\n",
    "                    ],\n",
    "                    \"normalize\": True,\n",
    "                },\n",
    "            }\n",
    "            for i in range(num_layers)\n",
    "        ]\n",
    "\n",
    "        blank_model_config = dict_overwrite(\n",
    "            get_model_config(model_id),\n",
    "            {\n",
    "                \"num_hidden_layers\": len(layer_assignments),\n",
    "            },\n",
    "        )\n",
    "        config = {\n",
    "            \"glue_task\": normalize_glue_task_name(model_id),\n",
    "            \"tokenizer_model_id\": model_id,\n",
    "            \"blank_model_config\": blank_model_config,\n",
    "            \"layer_assignments\": layer_assignments,\n",
    "            \"classification_head\": {\n",
    "                \"type\": \"SingleClassificationHead\",\n",
    "                \"params\": {\n",
    "                    \"donor\": model_id,\n",
    "                },\n",
    "            },\n",
    "            \"embeddings\": {\n",
    "                \"type\": \"SingleEmbeddings\",\n",
    "                \"params\": {\n",
    "                    \"donor\": model_id,\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "\n",
    "        yield config\n",
    "\n",
    "\n",
    "weave_configs = list(FisherAllLayers(model_id))\n",
    "\n",
    "scores = Parallel(n_jobs=5, return_as=\"list\")(\n",
    "    delayed(calculate_score_from_weaving_config_cached)(\n",
    "        weave_config,\n",
    "        # n_examples=4096,\n",
    "        n_examples=129,\n",
    "        split=\"validation\",\n",
    "    )\n",
    "    for weave_config in weave_configs\n",
    ")\n",
    "accuracies = [score[\"accuracy\"] for score in scores]\n",
    "\n",
    "records = []\n",
    "for weave_config, accuracy in zip(weave_configs, accuracies):\n",
    "    record = {}\n",
    "    record[\"name\"] = \"FisherAllLayers\"\n",
    "    record[\"accuracy\"] = accuracy\n",
    "    records.append(record)\n",
    "df_rte_vanilla = pd.DataFrame.from_records(records)\n",
    "df_rte_vanilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=128, split='validation')\n",
      "calculating score for weaving config md5sum: ee03e67dc871bb16971869b787d611db\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=128, split='validation')\n",
      "calculating score for weaving config md5sum: 435a935309220841446accf7dedbc392\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=128, split='validation')\n",
      "calculating score for weaving config md5sum: 452b777b802be27cb3474af6c0f2e96c\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=128, split='validation')\n",
      "calculating score for weaving config md5sum: e7767762e86c2914d3029a8b9a24fbea\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=128, split='validation')\n",
      "calculating score for weaving config md5sum: ca02b1f310fe746570247afe7e48cf73\n",
      "Loading textAttack/roberta-base-RTE\n",
      "Loading textAttack/roberta-base-RTE\n",
      "Loading textAttack/roberta-base-MNLI\n",
      "Loading textAttack/roberta-base-RTE\n",
      "Loading textAttack/roberta-base-MNLI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading textAttack/roberta-base-MNLI\n",
      "Loading textAttack/roberta-base-MNLI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading textAttack/roberta-base-RTE\n",
      "Loading textAttack/roberta-base-RTE\n",
      "Loading textAttack/roberta-base-MNLI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "/Users/briancruz/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "/Users/briancruz/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "/Users/briancruz/2023-fall-cs-194-294-merging-llms/model_merging/model_merging/evaluation.py:7: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  return hfds.load_metric(\"glue\", task)\n",
      "/Users/briancruz/2023-fall-cs-194-294-merging-llms/model_merging/model_merging/evaluation.py:7: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  return hfds.load_metric(\"glue\", task)\n",
      "/Users/briancruz/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "/Users/briancruz/2023-fall-cs-194-294-merging-llms/model_merging/model_merging/evaluation.py:7: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  return hfds.load_metric(\"glue\", task)\n",
      "/Users/briancruz/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "/Users/briancruz/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "/Users/briancruz/2023-fall-cs-194-294-merging-llms/model_merging/model_merging/evaluation.py:7: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  return hfds.load_metric(\"glue\", task)\n",
      "/Users/briancruz/2023-fall-cs-194-294-merging-llms/model_merging/model_merging/evaluation.py:7: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  return hfds.load_metric(\"glue\", task)\n",
      "2023-11-28 15:23:48.328989: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-11-28 15:23:49.350789: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-11-28 15:23:49.903638: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-11-28 15:23:49.922150: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-11-28 15:23:49.927714: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________________________calculate_score_from_weaving_config - 33.9s, 0.6min\n",
      "_____________________________calculate_score_from_weaving_config - 34.8s, 0.6min\n",
      "_____________________________calculate_score_from_weaving_config - 35.9s, 0.6min\n",
      "_____________________________calculate_score_from_weaving_config - 36.0s, 0.6min\n",
      "_____________________________calculate_score_from_weaving_config - 36.0s, 0.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/briancruz/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=128, split='validation')\n",
      "calculating score for weaving config md5sum: 0671b4be4ed4baaac504493a6e14f691\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=128, split='validation')\n",
      "calculating score for weaving config md5sum: 92b171244e8c79e698f95a5c54548ee1\n",
      "Loading textAttack/roberta-base-RTE\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=128, split='validation')\n",
      "calculating score for weaving config md5sum: 34b8e20e0399f89816db5be51f280afc\n",
      "Loading textAttack/roberta-base-RTE\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=128, split='validation')\n",
      "calculating score for weaving config md5sum: 3508278486073d3b2cd40249420bc4a1\n",
      "Loading textAttack/roberta-base-MNLI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading textAttack/roberta-base-MNLI\n",
      "Loading textAttack/roberta-base-MNLI\n",
      "Loading textAttack/roberta-base-MNLI\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=128, split='validation')\n",
      "calculating score for weaving config md5sum: 11dbf2a9fa39be14896870c99df7e534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading textAttack/roberta-base-RTE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading textAttack/roberta-base-RTE\n",
      "Loading textAttack/roberta-base-RTE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading textAttack/roberta-base-MNLI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/briancruz/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "/Users/briancruz/2023-fall-cs-194-294-merging-llms/model_merging/model_merging/evaluation.py:7: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  return hfds.load_metric(\"glue\", task)\n",
      "/Users/briancruz/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "/Users/briancruz/2023-fall-cs-194-294-merging-llms/model_merging/model_merging/evaluation.py:7: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  return hfds.load_metric(\"glue\", task)\n",
      "/Users/briancruz/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "/Users/briancruz/2023-fall-cs-194-294-merging-llms/model_merging/model_merging/evaluation.py:7: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  return hfds.load_metric(\"glue\", task)\n",
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "/Users/briancruz/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "/Users/briancruz/2023-fall-cs-194-294-merging-llms/model_merging/model_merging/evaluation.py:7: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  return hfds.load_metric(\"glue\", task)\n",
      "/Users/briancruz/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "/Users/briancruz/2023-fall-cs-194-294-merging-llms/model_merging/model_merging/evaluation.py:7: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  return hfds.load_metric(\"glue\", task)\n",
      "2023-11-28 15:24:24.507081: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-11-28 15:24:26.422313: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-11-28 15:24:26.485180: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-11-28 15:24:29.721939: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-11-28 15:24:32.311810: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________________________calculate_score_from_weaving_config - 30.6s, 0.5min\n",
      "_____________________________calculate_score_from_weaving_config - 32.0s, 0.5min\n",
      "_____________________________calculate_score_from_weaving_config - 31.4s, 0.5min\n",
      "_____________________________calculate_score_from_weaving_config - 31.9s, 0.5min\n",
      "_____________________________calculate_score_from_weaving_config - 31.3s, 0.5min\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FisherMARENSLayers</td>\n",
       "      <td>0.710938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FisherMARENSLayers</td>\n",
       "      <td>0.734375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FisherMARENSLayers</td>\n",
       "      <td>0.734375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FisherMARENSLayers</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FisherMARENSLayers</td>\n",
       "      <td>0.765625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>FisherMARENSLayers</td>\n",
       "      <td>0.765625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>FisherMARENSLayers</td>\n",
       "      <td>0.734375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>FisherMARENSLayers</td>\n",
       "      <td>0.726562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>FisherMARENSLayers</td>\n",
       "      <td>0.726562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>FisherMARENSLayers</td>\n",
       "      <td>0.726562</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 name  accuracy\n",
       "0  FisherMARENSLayers  0.710938\n",
       "1  FisherMARENSLayers  0.734375\n",
       "2  FisherMARENSLayers  0.734375\n",
       "3  FisherMARENSLayers  0.750000\n",
       "4  FisherMARENSLayers  0.765625\n",
       "5  FisherMARENSLayers  0.765625\n",
       "6  FisherMARENSLayers  0.734375\n",
       "7  FisherMARENSLayers  0.726562\n",
       "8  FisherMARENSLayers  0.726562\n",
       "9  FisherMARENSLayers  0.726562"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from llm_weaver import dict_overwrite, get_model_config, normalize_glue_task_name\n",
    "\n",
    "model_id = \"textAttack/roberta-base-RTE\"\n",
    "\n",
    "\n",
    "def FisherMARENSLayers(model_id):\n",
    "    replacement_layers = [0, 1, 4, 11]\n",
    "    for alpha in [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.7, 0.8, 0.9, 1.0]:\n",
    "        num_layers = get_model_config(model_id)[\"num_hidden_layers\"]\n",
    "        layer_assignments = [\n",
    "            (\n",
    "                {\n",
    "                    \"type\": \"ElementWiseLinearCombination\",\n",
    "                    \"params\": {\n",
    "                        \"donors\": [\n",
    "                            {\n",
    "                                \"donor\": model_id,\n",
    "                                \"hidden_layer_number\": i,\n",
    "                                \"weight\": alpha,\n",
    "                                \"element_wise_multiplier_filename\": f\"../data/fisher_info/{model_id.replace('/', '_')}-fisher-info.h5\",\n",
    "                            },\n",
    "                            {\n",
    "                                \"donor\": \"textAttack/roberta-base-MNLI\",\n",
    "                                \"hidden_layer_number\": i,\n",
    "                                \"weight\": 1.0 - alpha,\n",
    "                                \"element_wise_multiplier_filename\": \"../data/fisher_info/textAttack_roberta-base-MNLI-fisher-info.h5\",\n",
    "                            },\n",
    "                        ],\n",
    "                        \"normalize\": True,\n",
    "                    },\n",
    "                }\n",
    "                if (i in replacement_layers)\n",
    "                else {\n",
    "                    \"type\": \"SingleLayer\",\n",
    "                    \"params\": {\n",
    "                        \"donor\": model_id,\n",
    "                        \"hidden_layer_number\": i,\n",
    "                    },\n",
    "                }\n",
    "            )\n",
    "            for i in range(num_layers)\n",
    "        ]\n",
    "\n",
    "        blank_model_config = dict_overwrite(\n",
    "            get_model_config(model_id),\n",
    "            {\n",
    "                \"num_hidden_layers\": len(layer_assignments),\n",
    "            },\n",
    "        )\n",
    "        config = {\n",
    "            \"glue_task\": normalize_glue_task_name(model_id),\n",
    "            \"tokenizer_model_id\": model_id,\n",
    "            \"blank_model_config\": blank_model_config,\n",
    "            \"layer_assignments\": layer_assignments,\n",
    "            \"classification_head\": {\n",
    "                \"type\": \"SingleClassificationHead\",\n",
    "                \"params\": {\n",
    "                    \"donor\": model_id,\n",
    "                },\n",
    "            },\n",
    "            \"embeddings\": {\n",
    "                \"type\": \"SingleEmbeddings\",\n",
    "                \"params\": {\n",
    "                    \"donor\": model_id,\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "\n",
    "        yield config\n",
    "\n",
    "\n",
    "weave_configs = list(FisherMARENSLayers(model_id))\n",
    "\n",
    "scores = Parallel(n_jobs=5, return_as=\"list\")(\n",
    "    delayed(calculate_score_from_weaving_config_cached)(\n",
    "        weave_config,\n",
    "        # n_examples=4096,\n",
    "        n_examples=128,\n",
    "        split=\"validation\",\n",
    "    )\n",
    "    for weave_config in weave_configs\n",
    ")\n",
    "accuracies = [score[\"accuracy\"] for score in scores]\n",
    "\n",
    "records = []\n",
    "for weave_config, accuracy in zip(weave_configs, accuracies):\n",
    "    record = {}\n",
    "    record[\"name\"] = \"FisherMARENSLayers\"\n",
    "    record[\"accuracy\"] = accuracy\n",
    "    records.append(record)\n",
    "df_rte_vanilla = pd.DataFrame.from_records(records)\n",
    "df_rte_vanilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ray'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/briancruz/2023-fall-cs-194-294-merging-llms/notebooks/2023-11-28-intermediate-task-training.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/briancruz/2023-fall-cs-194-294-merging-llms/notebooks/2023-11-28-intermediate-task-training.ipynb#X31sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mray\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ray'"
     ]
    }
   ],
   "source": [
    "import ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=128, split='train')\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=128, split='train')\n",
      "calculating score for weaving config md5sum: 435a935309220841446accf7dedbc392\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=128, split='train')\n",
      "calculating score for weaving config md5sum: e7767762e86c2914d3029a8b9a24fbea\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=128, split='train')\n",
      "calculating score for weaving config md5sum: ee03e67dc871bb16971869b787d611db\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=128, split='train')\n",
      "calculating score for weaving config md5sum: ca02b1f310fe746570247afe7e48cf73\n",
      "calculating score for weaving config md5sum: 452b777b802be27cb3474af6c0f2e96c\n",
      "Loading textAttack/roberta-base-RTE\n",
      "Loading textAttack/roberta-base-MNLI\n",
      "Loading textAttack/roberta-base-MNLI\n",
      "Loading textAttack/roberta-base-MNLI\n",
      "Loading textAttack/roberta-base-MNLI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading textAttack/roberta-base-RTE\n",
      "Loading textAttack/roberta-base-MNLI\n",
      "Loading textAttack/roberta-base-RTE\n",
      "Loading textAttack/roberta-base-RTE\n",
      "Loading textAttack/roberta-base-RTE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "/Users/briancruz/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "/Users/briancruz/2023-fall-cs-194-294-merging-llms/model_merging/model_merging/evaluation.py:7: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  return hfds.load_metric(\"glue\", task)\n",
      "/Users/briancruz/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "/Users/briancruz/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "/Users/briancruz/2023-fall-cs-194-294-merging-llms/model_merging/model_merging/evaluation.py:7: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  return hfds.load_metric(\"glue\", task)\n",
      "/Users/briancruz/2023-fall-cs-194-294-merging-llms/model_merging/model_merging/evaluation.py:7: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  return hfds.load_metric(\"glue\", task)\n",
      "/Users/briancruz/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "/Users/briancruz/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "/Users/briancruz/2023-fall-cs-194-294-merging-llms/model_merging/model_merging/evaluation.py:7: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  return hfds.load_metric(\"glue\", task)\n",
      "/Users/briancruz/2023-fall-cs-194-294-merging-llms/model_merging/model_merging/evaluation.py:7: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  return hfds.load_metric(\"glue\", task)\n",
      "2023-11-28 16:22:26.631360: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-11-28 16:22:27.984096: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-11-28 16:22:28.028030: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-11-28 16:22:28.033351: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-11-28 16:22:28.355196: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________________________calculate_score_from_weaving_config - 36.1s, 0.6min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=128, split='train')\n",
      "calculating score for weaving config md5sum: 0671b4be4ed4baaac504493a6e14f691\n",
      "_____________________________calculate_score_from_weaving_config - 38.8s, 0.6min\n",
      "Loading textAttack/roberta-base-MNLI\n",
      "Loading textAttack/roberta-base-RTE\n",
      "_____________________________calculate_score_from_weaving_config - 38.9s, 0.6min\n",
      "_____________________________calculate_score_from_weaving_config - 38.9s, 0.6min\n",
      "_____________________________calculate_score_from_weaving_config - 38.9s, 0.6min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=128, split='train')\n",
      "calculating score for weaving config md5sum: 92b171244e8c79e698f95a5c54548ee1\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=128, split='train')\n",
      "calculating score for weaving config md5sum: 34b8e20e0399f89816db5be51f280afc\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=128, split='train')\n",
      "calculating score for weaving config md5sum: 3508278486073d3b2cd40249420bc4a1\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=128, split='train')\n",
      "calculating score for weaving config md5sum: 11dbf2a9fa39be14896870c99df7e534\n",
      "Loading textAttack/roberta-base-MNLI\n",
      "Loading textAttack/roberta-base-RTE\n",
      "Loading textAttack/roberta-base-RTE\n",
      "Loading textAttack/roberta-base-MNLI\n",
      "Loading textAttack/roberta-base-MNLI\n",
      "Loading textAttack/roberta-base-RTE\n",
      "Loading textAttack/roberta-base-MNLI\n",
      "Loading textAttack/roberta-base-RTE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/briancruz/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "/Users/briancruz/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "/Users/briancruz/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "/Users/briancruz/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "/Users/briancruz/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "2023-11-28 16:22:52.340178: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-11-28 16:22:57.270929: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-11-28 16:22:57.863617: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-11-28 16:22:57.941161: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-11-28 16:22:57.955305: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________________________calculate_score_from_weaving_config - 24.6s, 0.4min\n",
      "_____________________________calculate_score_from_weaving_config - 27.8s, 0.5min\n",
      "_____________________________calculate_score_from_weaving_config - 28.6s, 0.5min\n",
      "_____________________________calculate_score_from_weaving_config - 28.8s, 0.5min\n",
      "_____________________________calculate_score_from_weaving_config - 28.7s, 0.5min\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FisherMARENSLayers</td>\n",
       "      <td>0.742188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FisherMARENSLayers</td>\n",
       "      <td>0.734375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FisherMARENSLayers</td>\n",
       "      <td>0.734375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FisherMARENSLayers</td>\n",
       "      <td>0.757812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FisherMARENSLayers</td>\n",
       "      <td>0.796875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>FisherMARENSLayers</td>\n",
       "      <td>0.796875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>FisherMARENSLayers</td>\n",
       "      <td>0.781250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>FisherMARENSLayers</td>\n",
       "      <td>0.765625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>FisherMARENSLayers</td>\n",
       "      <td>0.773438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>FisherMARENSLayers</td>\n",
       "      <td>0.773438</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 name  accuracy\n",
       "0  FisherMARENSLayers  0.742188\n",
       "1  FisherMARENSLayers  0.734375\n",
       "2  FisherMARENSLayers  0.734375\n",
       "3  FisherMARENSLayers  0.757812\n",
       "4  FisherMARENSLayers  0.796875\n",
       "5  FisherMARENSLayers  0.796875\n",
       "6  FisherMARENSLayers  0.781250\n",
       "7  FisherMARENSLayers  0.765625\n",
       "8  FisherMARENSLayers  0.773438\n",
       "9  FisherMARENSLayers  0.773438"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from llm_weaver import dict_overwrite, get_model_config, normalize_glue_task_name\n",
    "\n",
    "model_id = \"textAttack/roberta-base-RTE\"\n",
    "\n",
    "\n",
    "def FisherMARENSLayers(model_id):\n",
    "    replacement_layers = [0, 1, 4, 11]\n",
    "    for alpha in [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.7, 0.8, 0.9, 1.0]:\n",
    "        num_layers = get_model_config(model_id)[\"num_hidden_layers\"]\n",
    "        layer_assignments = [\n",
    "            (\n",
    "                {\n",
    "                    \"type\": \"ElementWiseLinearCombination\",\n",
    "                    \"params\": {\n",
    "                        \"donors\": [\n",
    "                            {\n",
    "                                \"donor\": model_id,\n",
    "                                \"hidden_layer_number\": i,\n",
    "                                \"weight\": alpha,\n",
    "                                \"element_wise_multiplier_filename\": f\"../data/fisher_info/{model_id.replace('/', '_')}-fisher-info.h5\",\n",
    "                            },\n",
    "                            {\n",
    "                                \"donor\": \"textAttack/roberta-base-MNLI\",\n",
    "                                \"hidden_layer_number\": i,\n",
    "                                \"weight\": 1.0 - alpha,\n",
    "                                \"element_wise_multiplier_filename\": \"../data/fisher_info/textAttack_roberta-base-MNLI-fisher-info.h5\",\n",
    "                            },\n",
    "                        ],\n",
    "                        \"normalize\": True,\n",
    "                    },\n",
    "                }\n",
    "                if (i in replacement_layers)\n",
    "                else {\n",
    "                    \"type\": \"SingleLayer\",\n",
    "                    \"params\": {\n",
    "                        \"donor\": model_id,\n",
    "                        \"hidden_layer_number\": i,\n",
    "                    },\n",
    "                }\n",
    "            )\n",
    "            for i in range(num_layers)\n",
    "        ]\n",
    "\n",
    "        blank_model_config = dict_overwrite(\n",
    "            get_model_config(model_id),\n",
    "            {\n",
    "                \"num_hidden_layers\": len(layer_assignments),\n",
    "            },\n",
    "        )\n",
    "        config = {\n",
    "            \"glue_task\": normalize_glue_task_name(model_id),\n",
    "            \"tokenizer_model_id\": model_id,\n",
    "            \"blank_model_config\": blank_model_config,\n",
    "            \"layer_assignments\": layer_assignments,\n",
    "            \"classification_head\": {\n",
    "                \"type\": \"SingleClassificationHead\",\n",
    "                \"params\": {\n",
    "                    \"donor\": model_id,\n",
    "                },\n",
    "            },\n",
    "            \"embeddings\": {\n",
    "                \"type\": \"SingleEmbeddings\",\n",
    "                \"params\": {\n",
    "                    \"donor\": model_id,\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "\n",
    "        yield config\n",
    "\n",
    "\n",
    "weave_configs = list(FisherMARENSLayers(model_id))\n",
    "\n",
    "scores = Parallel(n_jobs=5, return_as=\"list\")(\n",
    "    delayed(calculate_score_from_weaving_config_cached)(\n",
    "        weave_config,\n",
    "        # n_examples=4096,\n",
    "        n_examples=128,\n",
    "        split=\"train\",\n",
    "    )\n",
    "    for weave_config in weave_configs\n",
    ")\n",
    "accuracies = [score[\"accuracy\"] for score in scores]\n",
    "\n",
    "records = []\n",
    "for weave_config, accuracy in zip(weave_configs, accuracies):\n",
    "    record = {}\n",
    "    record[\"name\"] = \"FisherMARENSLayers\"\n",
    "    record[\"accuracy\"] = accuracy\n",
    "    records.append(record)\n",
    "df_rte_vanilla = pd.DataFrame.from_records(records)\n",
    "df_rte_vanilla"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.18 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "418d7ffbf17f07fb7db812548652bd547b7709e294d6b5c17ab307303352fd7a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
